{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3730,
     "status": "ok",
     "timestamp": 1591625299463,
     "user": {
      "displayName": "Joel Joseph",
      "photoUrl": "",
      "userId": "10441793622284775762"
     },
     "user_tz": 0
    },
    "id": "K6u9L1KMQpZx",
    "outputId": "6087fccf-e914-459b-8ee3-672014266607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joel/Desktop/GVGAI_GYM/visionpack\n"
     ]
    }
   ],
   "source": [
    "%cd visionpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent.py\t       elianentity.zip\tnew_200.pkl\r\n",
      "__pycache__\t       monitor.csv\tnew_2000.pkl\r\n",
      "checkpoint_479999.pkl  new_1000.pkl\tstable_baselines3\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13193,
     "status": "ok",
     "timestamp": 1591637857406,
     "user": {
      "displayName": "Joel Joseph",
      "photoUrl": "",
      "userId": "10441793622284775762"
     },
     "user_tz": 0
    },
    "id": "HvmodhF5eKSH",
    "outputId": "6b70a328-d607-4b7d-c75a-f8c96a8275fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 37471 ...\n",
      "Client connected to server [OK]\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gym_gvgai\n",
    "import os\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from stable_baselines3 import PPO  # test arbitrary agent\n",
    "\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "  \"\"\"\n",
    "  Callback called at each step (for DQN and others) or after n steps (see ACER or PPO2)\n",
    "  :param _locals: (dict)\n",
    "  :param _globals: (dict)\n",
    "  \"\"\"\n",
    "  global n_steps, best_mean_reward\n",
    "  if (n_steps + 1) % 1000 == 0:\n",
    "      # Evaluate policy training performance\n",
    "#       if (n_steps + 1) % 10000 == 0:\n",
    "#           _locals['self'].save(log_dir + 'checkpoint'+str(n_steps)+'.pkl')\n",
    "      x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "      if len(x) > 0:\n",
    "          mean_reward = np.mean(y[-100:])\n",
    "          print(x[-1], 'timesteps')\n",
    "          print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "          # New best model, you could save the agent here\n",
    "          if mean_reward > best_mean_reward:\n",
    "              best_mean_reward = mean_reward\n",
    "              # Example for saving best model\n",
    "              print(\"Saving new best model\")\n",
    "              _locals['self'].save(log_dir + 'best_model_treasurekeeper_lvl0.pkl')\n",
    "  n_steps += 1\n",
    "  return True\n",
    "\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"./\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = gym_gvgai.make('gvgai-treasurekeeper-lvl0-v0')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=1)\n",
    "\n",
    "# model = PPO(CnnPolicy, env, verbose=1,prioritized_replay=True, buffer_size= 1000000, exploration_final_eps=0.1, train_freq=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0H2N8jv3Ps14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898 timesteps\n",
      "Best mean reward: -inf - Last mean reward per episode: 0.91\n",
      "Saving new best model\n",
      "1830 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.50\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 91.5         |\n",
      "|    ep_rew_mean          | 0.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036652384 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.906       |\n",
      "|    explained_variance   | -6.25e+13    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 4890         |\n",
      "|    policy_gradient_loss | -0.000504    |\n",
      "|    value_loss           | 0.89         |\n",
      "------------------------------------------\n",
      "2956 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.86\n",
      "4000 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.57\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 89.2       |\n",
      "|    ep_rew_mean          | 0.556      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 30         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 134        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01226087 |\n",
      "|    clip_fraction        | 0.0469     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -3.52      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.029      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00233   |\n",
      "|    value_loss           | 0.24       |\n",
      "----------------------------------------\n",
      "4784 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.48\n",
      "5951 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.41\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.5         |\n",
      "|    ep_rew_mean          | 0.397        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 208          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122418795 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -1.08e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0063      |\n",
      "|    value_loss           | 0.307        |\n",
      "------------------------------------------\n",
      "6951 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.44\n",
      "7722 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.42\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 109          |\n",
      "|    ep_rew_mean          | 0.423        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025033075 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | -8.1e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00458      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 3.62e-05     |\n",
      "------------------------------------------\n",
      "8938 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.39\n",
      "9837 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.37\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 121         |\n",
      "|    ep_rew_mean          | 0.361       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015018388 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -264        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00996    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 0.0124      |\n",
      "-----------------------------------------\n",
      "10860 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.33\n",
      "11972 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.32\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 124         |\n",
      "|    ep_rew_mean          | 0.306       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012012487 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -4.33e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00444    |\n",
      "|    value_loss           | 4.77e-06    |\n",
      "-----------------------------------------\n",
      "12817 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.25\n",
      "13910 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.20\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 129        |\n",
      "|    ep_rew_mean          | 0.2        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 495        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01943425 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -3.27e+06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00452   |\n",
      "|    value_loss           | 1.06e-05   |\n",
      "----------------------------------------\n",
      "14943 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.20\n",
      "15902 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 130         |\n",
      "|    ep_rew_mean          | 0.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 569         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011757291 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00718    |\n",
      "|    value_loss           | 1.78e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16991 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.05\n",
      "17879 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 122         |\n",
      "|    ep_rew_mean          | 0.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013336846 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | -4.33e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00964    |\n",
      "|    value_loss           | 1.05e-06    |\n",
      "-----------------------------------------\n",
      "18943 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "19998 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.8        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010211503 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | -2.34e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0243      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    value_loss           | 5.56e-07    |\n",
      "-----------------------------------------\n",
      "20987 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "21936 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 93.2       |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 788        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01616462 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | -4.75e+12  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0627     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00274   |\n",
      "|    value_loss           | 2.76e-07   |\n",
      "----------------------------------------\n",
      "22977 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "23978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 85          |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 864         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014247209 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00786    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    value_loss           | 9.14e-08    |\n",
      "-----------------------------------------\n",
      "24908 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "25968 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.3        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 939         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007024425 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -7.08e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0063      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000227   |\n",
      "|    value_loss           | 4.72e-08    |\n",
      "-----------------------------------------\n",
      "26998 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "27969 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 85.9        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1014        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015657866 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.31e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00179    |\n",
      "|    value_loss           | 2.29e-08    |\n",
      "-----------------------------------------\n",
      "28611 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "29859 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 89.2        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1089        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013252917 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00935    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00958    |\n",
      "|    value_loss           | 1.2e-08     |\n",
      "-----------------------------------------\n",
      "30768 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "31980 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.3        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1163        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012729752 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00758    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 3.83e-09    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32649 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "33752 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97.3        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1236        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017108835 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00506    |\n",
      "|    value_loss           | 1.55e-09    |\n",
      "-----------------------------------------\n",
      "34935 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "35954 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98           |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 1312         |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069390107 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.61e-05     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 4.04e-10     |\n",
      "------------------------------------------\n",
      "36576 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "37782 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 1387        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008408152 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00612    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 2.88e-10    |\n",
      "-----------------------------------------\n",
      "38774 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "39978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 111       |\n",
      "|    ep_rew_mean          | 0         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 28        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 1460      |\n",
      "|    total_timesteps      | 40960     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0124727 |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.33     |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0356    |\n",
      "|    n_updates            | 190       |\n",
      "|    policy_gradient_loss | -0.00217  |\n",
      "|    value_loss           | 4.82e-07  |\n",
      "---------------------------------------\n",
      "40796 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "41957 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "42990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 1532         |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023180922 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -2.94e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0217      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    value_loss           | 3.38e-05     |\n",
      "------------------------------------------\n",
      "43907 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "44888 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 106          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 1606         |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038255607 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -9e+12       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00132     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000613    |\n",
      "|    value_loss           | 1.05e-06     |\n",
      "------------------------------------------\n",
      "45906 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "46855 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1678        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008938342 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | -9.49e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 3.81e-05    |\n",
      "-----------------------------------------\n",
      "47860 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "48966 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 112        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 1751       |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01348297 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | -9.39e+11  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00425    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00181   |\n",
      "|    value_loss           | 2.61e-07   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49995 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "50841 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 111         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1824        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007761423 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -9.91e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.000104    |\n",
      "|    value_loss           | 1.81e-07    |\n",
      "-----------------------------------------\n",
      "51514 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "52918 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 122         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1896        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013718315 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -7.57e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 2.31e-05    |\n",
      "-----------------------------------------\n",
      "53590 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "54519 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 131          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1969         |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066115586 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | -7.05e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0118      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0049      |\n",
      "|    value_loss           | 2.71e-07     |\n",
      "------------------------------------------\n",
      "55813 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "56765 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 121          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 2042         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019041839 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | -1.36e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 1.6e-07      |\n",
      "------------------------------------------\n",
      "57849 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "58869 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 135         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 2115        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007856105 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -4.09e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.000852   |\n",
      "|    value_loss           | 1.67e-07    |\n",
      "-----------------------------------------\n",
      "59792 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "60986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 2187        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008785702 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -1.04e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00498    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    value_loss           | 3.57e-05    |\n",
      "-----------------------------------------\n",
      "61651 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "62914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 141          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 2259         |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041092816 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | -7.34e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00796      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000921    |\n",
      "|    value_loss           | 7.36e-09     |\n",
      "------------------------------------------\n",
      "63569 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "64762 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 147         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 2330        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007098752 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    value_loss           | 1.65e-09    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "66882 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 151           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 2403          |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029965187 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04         |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00221       |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -4.47e-05     |\n",
      "|    value_loss           | 1.68e-09      |\n",
      "-------------------------------------------\n",
      "67974 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "68997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 140         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 2477        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011322761 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00258    |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "69671 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "70989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 2549         |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025455505 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00662      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    value_loss           | 0.000121     |\n",
      "------------------------------------------\n",
      "71874 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "72983 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 161         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 2622        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004207703 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000175    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    value_loss           | 3.22e-09    |\n",
      "-----------------------------------------\n",
      "73511 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "74942 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 165          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 2695         |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030337744 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | 0.000327     |\n",
      "|    value_loss           | 1.03e-09     |\n",
      "------------------------------------------\n",
      "75940 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "76835 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 2767         |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025458806 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0353      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    value_loss           | 4.87e-10     |\n",
      "------------------------------------------\n",
      "77992 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "78853 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 2840        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004721093 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    value_loss           | 1.37e-10    |\n",
      "-----------------------------------------\n",
      "79993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "80604 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 2913        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015207371 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00689     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 3.21e-10    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "82880 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 2987        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013350463 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00697    |\n",
      "|    value_loss           | 3.14e-05    |\n",
      "-----------------------------------------\n",
      "83752 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "84990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "85523 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 3059         |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033080964 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | -7.35e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00998      |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000567    |\n",
      "|    value_loss           | 1.73e-06     |\n",
      "------------------------------------------\n",
      "86859 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "87671 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 3131        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015364495 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -1.38e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 2.33e-07    |\n",
      "-----------------------------------------\n",
      "88526 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "89997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 190        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 3203       |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01653057 |\n",
      "|    clip_fraction        | 0.0156     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | -5.86e+12  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00199   |\n",
      "|    value_loss           | 3.13e-06   |\n",
      "----------------------------------------\n",
      "90636 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "91879 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 196         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 3275        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011256525 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -9.46e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    value_loss           | 4.56e-05    |\n",
      "-----------------------------------------\n",
      "92891 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "93910 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 191          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 3347         |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149769075 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -2.43e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.027        |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 5.34e-08     |\n",
      "------------------------------------------\n",
      "94555 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "95989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 181         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 3419        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014650077 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -1.04e+03   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00769     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0073     |\n",
      "|    value_loss           | 4.24e-08    |\n",
      "-----------------------------------------\n",
      "96987 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "97612 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 180         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 3490        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009604633 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -8.67e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0341     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 4.37e-09    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98701 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "99989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 177          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 3563         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033500206 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0163      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 8.46e-09     |\n",
      "------------------------------------------\n",
      "100624 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "101963 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 167         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 3635        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010613116 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -5.58e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0387     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    value_loss           | 1.38e-05    |\n",
      "-----------------------------------------\n",
      "102840 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "103440 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 178         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 3706        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017911438 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -3.63e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00905     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    value_loss           | 3.69e-05    |\n",
      "-----------------------------------------\n",
      "104651 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "105967 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 179        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 3777       |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02535823 |\n",
      "|    clip_fraction        | 0.0312     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.31      |\n",
      "|    explained_variance   | -2.29e+12  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00127   |\n",
      "|    value_loss           | 3.76e-06   |\n",
      "----------------------------------------\n",
      "106620 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "107542 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 190          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 3849         |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048807194 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | -2.78e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0177       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00232     |\n",
      "|    value_loss           | 3.33e-06     |\n",
      "------------------------------------------\n",
      "108413 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "109613 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 205         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 3921        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015674608 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00548    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    value_loss           | 2.45e-06    |\n",
      "-----------------------------------------\n",
      "110865 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "111837 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 212         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 3992        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012527296 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | -4.38e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00779     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 1.9e-06     |\n",
      "-----------------------------------------\n",
      "112445 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "113741 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 225          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 4063         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044323793 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -2.4e+12     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0169       |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 4.58e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114801 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "115505 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 4134        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015080538 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -2.15e+10   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0293      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    value_loss           | 3.04e-06    |\n",
      "-----------------------------------------\n",
      "116977 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "117994 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 248         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 4205        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006284343 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00665     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 3e-06       |\n",
      "-----------------------------------------\n",
      "118594 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "119643 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 263         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 4277        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010350649 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0317      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 4.33e-06    |\n",
      "-----------------------------------------\n",
      "120843 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "121443 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 284         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 4349        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006641108 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -5.41e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0066      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.000713    |\n",
      "|    value_loss           | 1.62e-06    |\n",
      "-----------------------------------------\n",
      "122643 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "123843 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 289         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 4420        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008888372 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | -4.4e+11    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "124894 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "125942 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 297         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 4490        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018658947 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00502    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 6.85e-06    |\n",
      "-----------------------------------------\n",
      "126509 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "127793 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "128993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 314          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 4561         |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028038286 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00405      |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 2.64e-06     |\n",
      "------------------------------------------\n",
      "129593 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "130793 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 326          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 4632         |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072532822 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | -1.14e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00766     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000714    |\n",
      "|    value_loss           | 6.43e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "132593 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 343          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 4702         |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032978589 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.855       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00875      |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 4.78e-06     |\n",
      "------------------------------------------\n",
      "133793 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "134993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 354         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 4773        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007186695 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 7.21e-06    |\n",
      "-----------------------------------------\n",
      "135713 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "136861 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 369          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 4845         |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062622707 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.729       |\n",
      "|    explained_variance   | -2.8e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0313       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 7.66e-07     |\n",
      "------------------------------------------\n",
      "137461 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "138661 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 4915         |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066831587 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.638       |\n",
      "|    explained_variance   | -4.75e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0412       |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000377    |\n",
      "|    value_loss           | 1.47e-06     |\n",
      "------------------------------------------\n",
      "139861 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "140977 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 397          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 4985         |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017585717 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.596       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0018       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.000631    |\n",
      "|    value_loss           | 7.41e-06     |\n",
      "------------------------------------------\n",
      "141577 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "142777 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 419          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 5057         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041136956 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.545       |\n",
      "|    explained_variance   | -1.49e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0116      |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | 6.5e-05      |\n",
      "|    value_loss           | 2.58e-07     |\n",
      "------------------------------------------\n",
      "143705 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "144905 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 432         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 5129        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005324197 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | -3.04e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000793   |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 2.23e-07    |\n",
      "-----------------------------------------\n",
      "145717 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "146917 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 446          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 5200         |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016262616 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | -5.63e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00389     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    value_loss           | 1.11e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147757 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "148941 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 436          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 5272         |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012845051 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | -2.47e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00781      |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 2.11e-06     |\n",
      "------------------------------------------\n",
      "149705 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "150509 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 445          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 5345         |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002892553 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00162     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000188    |\n",
      "|    value_loss           | 1.68e-06     |\n",
      "------------------------------------------\n",
      "151633 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "152789 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 450          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 5416         |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015300349 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | -4e+11       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00389     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 1.85e-06     |\n",
      "------------------------------------------\n",
      "153921 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "154941 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 448           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 5488          |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048614724 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.257        |\n",
      "|    explained_variance   | -1.14e+12     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.018        |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | -0.00135      |\n",
      "|    value_loss           | 1.91e-06      |\n",
      "-------------------------------------------\n",
      "155409 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "156937 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 448          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 5560         |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011776084 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | -2.47e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00939     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.000753    |\n",
      "|    value_loss           | 7.84e-06     |\n",
      "------------------------------------------\n",
      "157909 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "158985 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 448          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 5632         |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006701433 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | -5.31e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00138     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | 0.000205     |\n",
      "|    value_loss           | 1.75e-08     |\n",
      "------------------------------------------\n",
      "159953 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "160825 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 447          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 5705         |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008668101 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0031      |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000713    |\n",
      "|    value_loss           | 7.27e-08     |\n",
      "------------------------------------------\n",
      "161709 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "162857 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 447          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 5778         |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002928254 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00452     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.000504    |\n",
      "|    value_loss           | 1.28e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163697 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "164825 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 442           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 81            |\n",
      "|    time_elapsed         | 5851          |\n",
      "|    total_timesteps      | 165888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027045037 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.419        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00529      |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | -0.00171      |\n",
      "|    value_loss           | 3.02e-06      |\n",
      "-------------------------------------------\n",
      "165617 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "166869 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 433          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 5923         |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001877587 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00229      |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00273     |\n",
      "|    value_loss           | 5.73e-06     |\n",
      "------------------------------------------\n",
      "167469 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "168669 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 447          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 5995         |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005448557 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | -5.16e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00194      |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | 1.74e-05     |\n",
      "|    value_loss           | 4.32e-07     |\n",
      "------------------------------------------\n",
      "169609 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "170705 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "171929 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 454         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 6068        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002660843 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.4        |\n",
      "|    explained_variance   | -2.89e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00663    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    value_loss           | 8.6e-07     |\n",
      "-----------------------------------------\n",
      "172645 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "173845 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 455          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 6141         |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037964778 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00125     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    value_loss           | 3.88e-06     |\n",
      "------------------------------------------\n",
      "174733 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "175933 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 451          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 6214         |\n",
      "|    total_timesteps      | 176128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012523667 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | -1.69e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00278     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.000418    |\n",
      "|    value_loss           | 8.54e-07     |\n",
      "------------------------------------------\n",
      "176533 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "177853 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 447         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 6287        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003525533 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.441      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00345     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    value_loss           | 4.75e-07    |\n",
      "-----------------------------------------\n",
      "178885 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "179988 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 438           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 88            |\n",
      "|    time_elapsed         | 6362          |\n",
      "|    total_timesteps      | 180224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052806886 |\n",
      "|    clip_fraction        | 0.0469        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.407        |\n",
      "|    explained_variance   | -3.42e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00149       |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | -0.000919     |\n",
      "|    value_loss           | 2.99e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180664 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "181558 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 423           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 6436          |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0015681259 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.386        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00254       |\n",
      "|    n_updates            | 880           |\n",
      "|    policy_gradient_loss | -0.000807     |\n",
      "|    value_loss           | 4.77e-10      |\n",
      "-------------------------------------------\n",
      "182722 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "183774 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 422          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 6509         |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017393474 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00018      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 2.96e-10     |\n",
      "------------------------------------------\n",
      "184882 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "185538 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 412          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 6583         |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022374098 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000858    |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 1.69e-10     |\n",
      "------------------------------------------\n",
      "186674 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "187681 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 420          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 6656         |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038995626 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.497       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00346     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 2.36e-10     |\n",
      "------------------------------------------\n",
      "188621 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "189665 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 425          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 6729         |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020839882 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00695     |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 3.59e-08     |\n",
      "------------------------------------------\n",
      "190865 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "191898 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 427            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 28             |\n",
      "|    iterations           | 94             |\n",
      "|    time_elapsed         | 6803           |\n",
      "|    total_timesteps      | 192512         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00015203143 |\n",
      "|    clip_fraction        | 0.0469         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.426         |\n",
      "|    explained_variance   | -9.28e+11      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00276       |\n",
      "|    n_updates            | 930            |\n",
      "|    policy_gradient_loss | -0.0019        |\n",
      "|    value_loss           | 1.36e-06       |\n",
      "--------------------------------------------\n",
      "192870 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "193982 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 417          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 6876         |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022168844 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.421       |\n",
      "|    explained_variance   | -2.95e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.000748    |\n",
      "|    value_loss           | 5.21e-06     |\n",
      "------------------------------------------\n",
      "194582 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "195782 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 424          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 6949         |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046374006 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | -4.33e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00164     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 1.12e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196830 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "197770 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 430          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 7023         |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018651726 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | -3.21e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00643      |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    value_loss           | 8.9e-06      |\n",
      "------------------------------------------\n",
      "198762 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "199686 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 429          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 7096         |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012642379 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.497       |\n",
      "|    explained_variance   | -2.83e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000272     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000412    |\n",
      "|    value_loss           | 9.45e-09     |\n",
      "------------------------------------------\n",
      "200874 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "201610 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 432         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 7169        |\n",
      "|    total_timesteps      | 202752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002441675 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | -4.79e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000916    |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    value_loss           | 4.06e-09    |\n",
      "-----------------------------------------\n",
      "202810 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "203970 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 433         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 7241        |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003639365 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | -6.52e+06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.000832   |\n",
      "|    value_loss           | 1.24e-08    |\n",
      "-----------------------------------------\n",
      "204726 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "205914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 425         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 7314        |\n",
      "|    total_timesteps      | 206848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002018774 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.000695   |\n",
      "|    value_loss           | 3.23e-06    |\n",
      "-----------------------------------------\n",
      "206738 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "207938 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 429          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 7385         |\n",
      "|    total_timesteps      | 208896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020862878 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00681     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.000993    |\n",
      "|    value_loss           | 2.24e-07     |\n",
      "------------------------------------------\n",
      "208538 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "209886 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 432          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 7458         |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016803225 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00126     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 3.96e-06     |\n",
      "------------------------------------------\n",
      "210686 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "211886 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 430          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 7531         |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031839586 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.523       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | 0.000406     |\n",
      "|    value_loss           | 1.8e-06      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212574 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "213874 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "214882 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 422         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 7603        |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003798413 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | -4.53e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00192    |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | 0.000301    |\n",
      "|    value_loss           | 1.85e-05    |\n",
      "-----------------------------------------\n",
      "215446 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "216458 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 423          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 7675         |\n",
      "|    total_timesteps      | 217088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024624201 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.527       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00106      |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 5.22e-07     |\n",
      "------------------------------------------\n",
      "217658 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "218858 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 423            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 28             |\n",
      "|    iterations           | 107            |\n",
      "|    time_elapsed         | 7746           |\n",
      "|    total_timesteps      | 219136         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00084225286 |\n",
      "|    clip_fraction        | 0.0156         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.588         |\n",
      "|    explained_variance   | -1.82e+12      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00449       |\n",
      "|    n_updates            | 1060           |\n",
      "|    policy_gradient_loss | -0.00187       |\n",
      "|    value_loss           | 1.83e-07       |\n",
      "--------------------------------------------\n",
      "219549 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "220749 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 423          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 7818         |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043391036 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | -9.02e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00244      |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "221953 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "222665 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 427          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 7890         |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020142016 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00401      |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 3.75e-07     |\n",
      "------------------------------------------\n",
      "223589 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "224645 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 437          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 7962         |\n",
      "|    total_timesteps      | 225280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045570494 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | -2.18e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000686    |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 2.51e-06     |\n",
      "------------------------------------------\n",
      "225845 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "226445 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 443         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 8033        |\n",
      "|    total_timesteps      | 227328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000580912 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | -2.81e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00661    |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.000968   |\n",
      "|    value_loss           | 1.97e-06    |\n",
      "-----------------------------------------\n",
      "227809 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "228933 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 444           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 112           |\n",
      "|    time_elapsed         | 8104          |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081223855 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.557        |\n",
      "|    explained_variance   | -6.68e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00279      |\n",
      "|    n_updates            | 1110          |\n",
      "|    policy_gradient_loss | -0.00114      |\n",
      "|    value_loss           | 1.5e-05       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229897 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "230497 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 450          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 8175         |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024974416 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | -2.35e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00171     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | 0.000121     |\n",
      "|    value_loss           | 1.66e-08     |\n",
      "------------------------------------------\n",
      "231942 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "232978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 436           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 114           |\n",
      "|    time_elapsed         | 8247          |\n",
      "|    total_timesteps      | 233472        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0021407735 |\n",
      "|    clip_fraction        | 0.125         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.581        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000349      |\n",
      "|    n_updates            | 1130          |\n",
      "|    policy_gradient_loss | -0.00182      |\n",
      "|    value_loss           | 2.7e-08       |\n",
      "-------------------------------------------\n",
      "233686 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "234922 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 430            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 28             |\n",
      "|    iterations           | 115            |\n",
      "|    time_elapsed         | 8318           |\n",
      "|    total_timesteps      | 235520         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00018731214 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.569         |\n",
      "|    explained_variance   | nan            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 0.00241        |\n",
      "|    n_updates            | 1140           |\n",
      "|    policy_gradient_loss | -0.00188       |\n",
      "|    value_loss           | 2.73e-08       |\n",
      "--------------------------------------------\n",
      "235690 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "236798 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 438          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 8389         |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027013663 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.569       |\n",
      "|    explained_variance   | -1.14e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00183      |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 9.81e-08     |\n",
      "------------------------------------------\n",
      "237850 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "238450 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 448         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 8459        |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002947336 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | -2.76e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00913    |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 2.81e-06    |\n",
      "-----------------------------------------\n",
      "239942 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "240890 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 439           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 118           |\n",
      "|    time_elapsed         | 8531          |\n",
      "|    total_timesteps      | 241664        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045506202 |\n",
      "|    clip_fraction        | 0.109         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.581        |\n",
      "|    explained_variance   | -2.23e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00303       |\n",
      "|    n_updates            | 1170          |\n",
      "|    policy_gradient_loss | -0.00179      |\n",
      "|    value_loss           | 5.25e-07      |\n",
      "-------------------------------------------\n",
      "241658 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "242718 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 446          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 8601         |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040833955 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00483     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 2.21e-06     |\n",
      "------------------------------------------\n",
      "243918 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "244586 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 444          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 8673         |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046727434 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.708       |\n",
      "|    explained_variance   | -8.3e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00499      |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -4.92e-06    |\n",
      "|    value_loss           | 1.67e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245786 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "246986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 449         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 8745        |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003243446 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | -2.93e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00632    |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 5.96e-08    |\n",
      "-----------------------------------------\n",
      "247714 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "248914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 453         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 8816        |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006820388 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | -5.19e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00294    |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.00444    |\n",
      "|    value_loss           | 4.77e-08    |\n",
      "-----------------------------------------\n",
      "249774 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "250538 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 457          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 8887         |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045143915 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.618       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0057       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "251630 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "252830 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 467          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 124          |\n",
      "|    time_elapsed         | 8959         |\n",
      "|    total_timesteps      | 253952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007578782 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.642       |\n",
      "|    explained_variance   | -1.76e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -9.64e-05    |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | -0.000565    |\n",
      "|    value_loss           | 3.36e-06     |\n",
      "------------------------------------------\n",
      "253430 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "254979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "255985 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 467          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 125          |\n",
      "|    time_elapsed         | 9030         |\n",
      "|    total_timesteps      | 256000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036538776 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.688       |\n",
      "|    explained_variance   | -3.14e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0148      |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    value_loss           | 1.54e-08     |\n",
      "------------------------------------------\n",
      "256585 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "257785 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 471          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 9101         |\n",
      "|    total_timesteps      | 258048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054026432 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.783       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00496     |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | -0.000625    |\n",
      "|    value_loss           | 6.67e-06     |\n",
      "------------------------------------------\n",
      "258831 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "259431 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 463          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 127          |\n",
      "|    time_elapsed         | 9173         |\n",
      "|    total_timesteps      | 260096       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020144547 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.822       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0022      |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | 0.000155     |\n",
      "|    value_loss           | 1.86e-07     |\n",
      "------------------------------------------\n",
      "260983 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "261947 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 474          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 9244         |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053739976 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.821       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00224      |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.000826    |\n",
      "|    value_loss           | 1.64e-08     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262547 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "263747 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 477          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 129          |\n",
      "|    time_elapsed         | 9315         |\n",
      "|    total_timesteps      | 264192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063867057 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.783       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00184      |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 6.33e-06     |\n",
      "------------------------------------------\n",
      "264627 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "265827 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 476          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 9387         |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016150014 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.871       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00771      |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 1.37e-07     |\n",
      "------------------------------------------\n",
      "266651 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "267851 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 475          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 9458         |\n",
      "|    total_timesteps      | 268288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026448998 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.825       |\n",
      "|    explained_variance   | -7.36e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0272      |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00341     |\n",
      "|    value_loss           | 4.94e-08     |\n",
      "------------------------------------------\n",
      "268427 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "269983 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 469         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 9530        |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007301194 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.892      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00456     |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.000556   |\n",
      "|    value_loss           | 4.35e-06    |\n",
      "-----------------------------------------\n",
      "270497 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "271827 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 466         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 9601        |\n",
      "|    total_timesteps      | 272384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006364025 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -7.6e+11    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    value_loss           | 2.75e-07    |\n",
      "-----------------------------------------\n",
      "272699 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "273631 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 458         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 9673        |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009971795 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -1.6e+12    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00408    |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    value_loss           | 5.26e-06    |\n",
      "-----------------------------------------\n",
      "274831 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "275431 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 461          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 135          |\n",
      "|    time_elapsed         | 9744         |\n",
      "|    total_timesteps      | 276480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028452673 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0192      |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    value_loss           | 1.34e-07     |\n",
      "------------------------------------------\n",
      "276849 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "277935 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 458         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 9816        |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011258107 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    value_loss           | 3.9e-08     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278835 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "279940 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 454         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 137         |\n",
      "|    time_elapsed         | 9890        |\n",
      "|    total_timesteps      | 280576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012200978 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00262    |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 3.06e-06    |\n",
      "-----------------------------------------\n",
      "280540 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "281740 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 447        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 138        |\n",
      "|    time_elapsed         | 9964       |\n",
      "|    total_timesteps      | 282624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01895807 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 1370       |\n",
      "|    policy_gradient_loss | -0.00223   |\n",
      "|    value_loss           | 2.14e-05   |\n",
      "----------------------------------------\n",
      "282890 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "283470 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 445         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 10036       |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011151195 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.466      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0053     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    value_loss           | 1.82e-06    |\n",
      "-----------------------------------------\n",
      "284474 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "285731 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 438         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 10109       |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013693254 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -3.47e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0207      |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    value_loss           | 1.43e-06    |\n",
      "-----------------------------------------\n",
      "286938 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "287762 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 432         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 141         |\n",
      "|    time_elapsed         | 10181       |\n",
      "|    total_timesteps      | 288768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023757935 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -1.82e+08   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.00717    |\n",
      "|    value_loss           | 1.29e-06    |\n",
      "-----------------------------------------\n",
      "288822 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "289422 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 428        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 142        |\n",
      "|    time_elapsed         | 10254      |\n",
      "|    total_timesteps      | 290816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01583107 |\n",
      "|    clip_fraction        | 0.0781     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | -0.00349   |\n",
      "|    value_loss           | 5.59e-06   |\n",
      "----------------------------------------\n",
      "290482 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "291710 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 425          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 143          |\n",
      "|    time_elapsed         | 10327        |\n",
      "|    total_timesteps      | 292864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072936537 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | -9.56e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0064       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 3.44e-07     |\n",
      "------------------------------------------\n",
      "292714 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "293941 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 422         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 10399       |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016138751 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -5.04e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00431     |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00798    |\n",
      "|    value_loss           | 2.1e-06     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294998 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "295905 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 405         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 10471       |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004932112 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -8.51e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "296505 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "297769 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "298737 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 146        |\n",
      "|    time_elapsed         | 10543      |\n",
      "|    total_timesteps      | 299008     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00930774 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | -3.2e+12   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 1450       |\n",
      "|    policy_gradient_loss | -0.00141   |\n",
      "|    value_loss           | 6.28e-08   |\n",
      "----------------------------------------\n",
      "299767 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "300608 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 147         |\n",
      "|    time_elapsed         | 10615       |\n",
      "|    total_timesteps      | 301056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019511357 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 4.69e-07    |\n",
      "-----------------------------------------\n",
      "301797 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "302877 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 403         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 10687       |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016047109 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -1.06e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.00913    |\n",
      "|    value_loss           | 4.31e-08    |\n",
      "-----------------------------------------\n",
      "303477 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "304719 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 401         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 149         |\n",
      "|    time_elapsed         | 10759       |\n",
      "|    total_timesteps      | 305152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008206675 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -3.05e+08   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00706    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.000998   |\n",
      "|    value_loss           | 1.28e-05    |\n",
      "-----------------------------------------\n",
      "305915 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "306804 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 393         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 10832       |\n",
      "|    total_timesteps      | 307200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012090918 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | -2.75e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00331    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 1.26e-06    |\n",
      "-----------------------------------------\n",
      "307823 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "308423 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 389         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 151         |\n",
      "|    time_elapsed         | 10905       |\n",
      "|    total_timesteps      | 309248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011093311 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | -4.52e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    value_loss           | 1.79e-07    |\n",
      "-----------------------------------------\n",
      "309924 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "310868 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 384        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 152        |\n",
      "|    time_elapsed         | 10977      |\n",
      "|    total_timesteps      | 311296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00484139 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0056     |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | -0.000973  |\n",
      "|    value_loss           | 2.52e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311864 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "312464 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 388          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 153          |\n",
      "|    time_elapsed         | 11050        |\n",
      "|    total_timesteps      | 313344       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049976166 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -6.24e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00807     |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 7.15e-07     |\n",
      "------------------------------------------\n",
      "313664 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "314514 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 383          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 11122        |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096036475 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -2.88e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0195       |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | 0.000396     |\n",
      "|    value_loss           | 5.35e-07     |\n",
      "------------------------------------------\n",
      "315775 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "316994 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 391        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 155        |\n",
      "|    time_elapsed         | 11194      |\n",
      "|    total_timesteps      | 317440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00663775 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | -5.89e+11  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0256     |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.00446   |\n",
      "|    value_loss           | 2.2e-06    |\n",
      "----------------------------------------\n",
      "317733 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "318865 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 401         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 11267       |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009012906 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | -5.98e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    value_loss           | 2.46e-06    |\n",
      "-----------------------------------------\n",
      "319413 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "320445 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 405         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 157         |\n",
      "|    time_elapsed         | 11340       |\n",
      "|    total_timesteps      | 321536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005816115 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00148    |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.000924   |\n",
      "|    value_loss           | 1.85e-07    |\n",
      "-----------------------------------------\n",
      "321645 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "322845 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 410         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 11413       |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007851986 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.807      |\n",
      "|    explained_variance   | -1.77e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0057     |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    value_loss           | 2.5e-06     |\n",
      "-----------------------------------------\n",
      "323550 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "324990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 414          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 159          |\n",
      "|    time_elapsed         | 11485        |\n",
      "|    total_timesteps      | 325632       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010338218 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.759       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00756      |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | 0.000354     |\n",
      "|    value_loss           | 1.71e-08     |\n",
      "------------------------------------------\n",
      "325439 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "326648 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 418          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 11558        |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043430394 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.742       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00857      |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.000583    |\n",
      "|    value_loss           | 3.42e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327556 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "328840 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 423           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 161           |\n",
      "|    time_elapsed         | 11630         |\n",
      "|    total_timesteps      | 329728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.7372726e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.726        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00157      |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | -0.000279     |\n",
      "|    value_loss           | 2.12e-07      |\n",
      "-------------------------------------------\n",
      "329440 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "330939 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 414          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 11705        |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059065446 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.653       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0101      |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | 0.000217     |\n",
      "|    value_loss           | 1.38e-07     |\n",
      "------------------------------------------\n",
      "331463 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "332808 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 413          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 163          |\n",
      "|    time_elapsed         | 11780        |\n",
      "|    total_timesteps      | 333824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016570558 |\n",
      "|    clip_fraction        | 0.188        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.666       |\n",
      "|    explained_variance   | -2.56e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00298      |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 4.14e-06     |\n",
      "------------------------------------------\n",
      "333650 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "334534 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 413          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 11855        |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005625385 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.709       |\n",
      "|    explained_variance   | -9.14e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00369      |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    value_loss           | 1.62e-06     |\n",
      "------------------------------------------\n",
      "335982 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "336950 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 406          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 165          |\n",
      "|    time_elapsed         | 11931        |\n",
      "|    total_timesteps      | 337920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024590222 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.798       |\n",
      "|    explained_variance   | -7.86e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 1.43e-06     |\n",
      "------------------------------------------\n",
      "337875 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "338975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 407           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 166           |\n",
      "|    time_elapsed         | 12006         |\n",
      "|    total_timesteps      | 339968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0001092396 |\n",
      "|    clip_fraction        | 0.0625        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.855        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00633      |\n",
      "|    n_updates            | 1650          |\n",
      "|    policy_gradient_loss | 0.00059       |\n",
      "|    value_loss           | 2.03e-08      |\n",
      "-------------------------------------------\n",
      "339415 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "340786 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "341706 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 382          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 167          |\n",
      "|    time_elapsed         | 12082        |\n",
      "|    total_timesteps      | 342016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039647254 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.913       |\n",
      "|    explained_variance   | -1.45e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00239     |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | 0.000758     |\n",
      "|    value_loss           | 1.98e-05     |\n",
      "------------------------------------------\n",
      "342967 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "343524 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 382         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 12157       |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008496541 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.887      |\n",
      "|    explained_variance   | -4.55e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00399     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 1.25e-08    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "345966 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 387          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 12232        |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034087934 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.943       |\n",
      "|    explained_variance   | -2.45e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00249     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | 9.65e-05     |\n",
      "|    value_loss           | 7.27e-09     |\n",
      "------------------------------------------\n",
      "346737 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "347731 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 388        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 170        |\n",
      "|    time_elapsed         | 12308      |\n",
      "|    total_timesteps      | 348160     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00278552 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.967     |\n",
      "|    explained_variance   | -6e+11     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0248     |\n",
      "|    n_updates            | 1690       |\n",
      "|    policy_gradient_loss | -0.00267   |\n",
      "|    value_loss           | 3.83e-09   |\n",
      "----------------------------------------\n",
      "348779 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "349639 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 388          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 12382        |\n",
      "|    total_timesteps      | 350208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060336282 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.955       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00628      |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.000859    |\n",
      "|    value_loss           | 5.38e-09     |\n",
      "------------------------------------------\n",
      "350828 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "351844 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 371         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 12457       |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007028839 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00607     |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 5.88e-08    |\n",
      "-----------------------------------------\n",
      "352877 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "353929 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 352         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 12532       |\n",
      "|    total_timesteps      | 354304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002137169 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | -4.73e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000395    |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 3.3e-06     |\n",
      "-----------------------------------------\n",
      "354879 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "355853 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 311          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 12608        |\n",
      "|    total_timesteps      | 356352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067304936 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -3.93e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    value_loss           | 2.23e-05     |\n",
      "------------------------------------------\n",
      "356921 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "357808 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 299         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 12684       |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003047998 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00313    |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    value_loss           | 5.36e-08    |\n",
      "-----------------------------------------\n",
      "358850 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "359939 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 286          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 176          |\n",
      "|    time_elapsed         | 12759        |\n",
      "|    total_timesteps      | 360448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035532247 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | -4.39e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    value_loss           | 5.72e-08     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360895 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "361800 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 240          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 12835        |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010578581 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | -3.9e+12     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00206      |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.000457    |\n",
      "|    value_loss           | 3.25e-08     |\n",
      "------------------------------------------\n",
      "362886 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "363997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 215         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 12911       |\n",
      "|    total_timesteps      | 364544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003948051 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | -6.98e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | 0.000176    |\n",
      "|    value_loss           | 8.54e-08    |\n",
      "-----------------------------------------\n",
      "364962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "365988 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 199          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 12984        |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051168124 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -7.01e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00692     |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 1.15e-07     |\n",
      "------------------------------------------\n",
      "366901 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "367501 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 205          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 13058        |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076084663 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.976       |\n",
      "|    explained_variance   | -1.15e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0177       |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -0.000882    |\n",
      "|    value_loss           | 5.45e-06     |\n",
      "------------------------------------------\n",
      "368768 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "369696 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 195          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 13132        |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039650863 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.917       |\n",
      "|    explained_variance   | -6.84e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0126      |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 4.18e-08     |\n",
      "------------------------------------------\n",
      "370759 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "371915 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 180         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 13208       |\n",
      "|    total_timesteps      | 372736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008609229 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | -5.18e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "372409 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "373946 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 13285        |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038744456 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00385     |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.00425     |\n",
      "|    value_loss           | 8.18e-09     |\n",
      "------------------------------------------\n",
      "374816 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "375876 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 157         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 13361       |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007952236 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -6.38e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0207     |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.0091     |\n",
      "|    value_loss           | 1.17e-08    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "377686 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 162        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 185        |\n",
      "|    time_elapsed         | 13438      |\n",
      "|    total_timesteps      | 378880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02763883 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0332     |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    value_loss           | 9.84e-09   |\n",
      "----------------------------------------\n",
      "378815 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "379930 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 186          |\n",
      "|    time_elapsed         | 13515        |\n",
      "|    total_timesteps      | 380928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012533676 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0416       |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.000171    |\n",
      "|    value_loss           | 1.32e-09     |\n",
      "------------------------------------------\n",
      "380952 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "381748 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 87.8         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 13593        |\n",
      "|    total_timesteps      | 382976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061144875 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0368      |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0068      |\n",
      "|    value_loss           | 1.46e-06     |\n",
      "------------------------------------------\n",
      "382983 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "383997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "384935 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 66.1        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 13671       |\n",
      "|    total_timesteps      | 385024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003793893 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -1.13e+13   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00454    |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    value_loss           | 9.02e-07    |\n",
      "-----------------------------------------\n",
      "385925 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "386803 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.4        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 13748       |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014455669 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -1.55e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00213     |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 5.39e-06    |\n",
      "-----------------------------------------\n",
      "387854 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "388839 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 68.6        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 13825       |\n",
      "|    total_timesteps      | 389120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008553421 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | -3.31e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00193    |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    value_loss           | 5.61e-07    |\n",
      "-----------------------------------------\n",
      "389961 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "390988 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.3        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 191         |\n",
      "|    time_elapsed         | 13905       |\n",
      "|    total_timesteps      | 391168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016386613 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -1.32e+13   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    value_loss           | 2.62e-06    |\n",
      "-----------------------------------------\n",
      "391404 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "392979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 57.8        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 192         |\n",
      "|    time_elapsed         | 13983       |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015752245 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | -4.81e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0328     |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    value_loss           | 2.83e-07    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393675 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "394922 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73.9        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 193         |\n",
      "|    time_elapsed         | 14060       |\n",
      "|    total_timesteps      | 395264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011270683 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -1.48e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.00953    |\n",
      "|    value_loss           | 4.55e-06    |\n",
      "-----------------------------------------\n",
      "395775 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "396975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 67.6        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 14140       |\n",
      "|    total_timesteps      | 397312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007093608 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -6.85e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0165     |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 9.15e-07    |\n",
      "-----------------------------------------\n",
      "397993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "398570 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 64.3        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 14223       |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012432518 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -1.1e+13    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0294     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.00863    |\n",
      "|    value_loss           | 4.85e-05    |\n",
      "-----------------------------------------\n",
      "399991 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "400980 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 57           |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 196          |\n",
      "|    time_elapsed         | 14304        |\n",
      "|    total_timesteps      | 401408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038674404 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | -1.38e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0233       |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | 1.85e-07     |\n",
      "|    value_loss           | 4.81e-07     |\n",
      "------------------------------------------\n",
      "401854 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "403000 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 48.7        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 197         |\n",
      "|    time_elapsed         | 14386       |\n",
      "|    total_timesteps      | 403456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014968367 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -1.08e+13   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 2.17e-07    |\n",
      "-----------------------------------------\n",
      "403988 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "404930 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 52           |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 198          |\n",
      "|    time_elapsed         | 14467        |\n",
      "|    total_timesteps      | 405504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032145698 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -1.96e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0103      |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    value_loss           | 9.19e-08     |\n",
      "------------------------------------------\n",
      "405643 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "406610 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.4        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 14547       |\n",
      "|    total_timesteps      | 407552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004621366 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -6.25e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    value_loss           | 4.23e-08    |\n",
      "-----------------------------------------\n",
      "407440 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "408871 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.8        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 14628       |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018181635 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | -2.25e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00266    |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 2.31e-08    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409847 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "410969 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.6        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 14704       |\n",
      "|    total_timesteps      | 411648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012504311 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -1.27e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0121      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 2.08e-08    |\n",
      "-----------------------------------------\n",
      "411983 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "412899 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 87.2        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 14783       |\n",
      "|    total_timesteps      | 413696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019426767 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | -1.09e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00502    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 1.45e-08    |\n",
      "-----------------------------------------\n",
      "413875 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "414791 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 83.1        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 203         |\n",
      "|    time_elapsed         | 14862       |\n",
      "|    total_timesteps      | 415744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008710621 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.858      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0032      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 2.93e-08    |\n",
      "-----------------------------------------\n",
      "415803 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "416798 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 87.7         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 204          |\n",
      "|    time_elapsed         | 14941        |\n",
      "|    total_timesteps      | 417792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013902764 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.875       |\n",
      "|    explained_variance   | -1.11e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0204       |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.000332    |\n",
      "|    value_loss           | 1e-06        |\n",
      "------------------------------------------\n",
      "417989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "418980 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 69.4        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 15023       |\n",
      "|    total_timesteps      | 419840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006155322 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.828      |\n",
      "|    explained_variance   | -7.32e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    value_loss           | 3.69e-06    |\n",
      "-----------------------------------------\n",
      "419954 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "420856 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 82.1         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 206          |\n",
      "|    time_elapsed         | 15100        |\n",
      "|    total_timesteps      | 421888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031695883 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.83        |\n",
      "|    explained_variance   | -1.73e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00311     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 2.1e-06      |\n",
      "------------------------------------------\n",
      "421745 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "422929 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 84.8         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 15178        |\n",
      "|    total_timesteps      | 423936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052233553 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.807       |\n",
      "|    explained_variance   | -1.47e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00363     |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 2.69e-06     |\n",
      "------------------------------------------\n",
      "424000 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "424997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 74.1         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 15259        |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059483657 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.89        |\n",
      "|    explained_variance   | -2.95e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0304       |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    value_loss           | 4.12e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425569 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "426991 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "427997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 80.4         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 209          |\n",
      "|    time_elapsed         | 15340        |\n",
      "|    total_timesteps      | 428032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025301958 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.815       |\n",
      "|    explained_variance   | -9.58e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0328       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 2.41e-07     |\n",
      "------------------------------------------\n",
      "428581 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "429932 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 71.9         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 210          |\n",
      "|    time_elapsed         | 15418        |\n",
      "|    total_timesteps      | 430080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062567126 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.865       |\n",
      "|    explained_variance   | -6.38e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0131      |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | -0.000812    |\n",
      "|    value_loss           | 3.19e-05     |\n",
      "------------------------------------------\n",
      "430552 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "431810 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 76.1        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 15498       |\n",
      "|    total_timesteps      | 432128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004833511 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00952    |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 4.62e-09    |\n",
      "-----------------------------------------\n",
      "432697 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "433774 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 87.7         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 212          |\n",
      "|    time_elapsed         | 15576        |\n",
      "|    total_timesteps      | 434176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077464427 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | -1.04e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00438     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    value_loss           | 4.73e-09     |\n",
      "------------------------------------------\n",
      "434562 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "435654 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90.5         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 213          |\n",
      "|    time_elapsed         | 15656        |\n",
      "|    total_timesteps      | 436224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034796305 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.991       |\n",
      "|    explained_variance   | -5.37e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00246     |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 2.41e-09     |\n",
      "------------------------------------------\n",
      "436988 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "437800 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 85.6         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 214          |\n",
      "|    time_elapsed         | 15736        |\n",
      "|    total_timesteps      | 438272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072794855 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.979       |\n",
      "|    explained_variance   | -1.9e+12     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0126       |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    value_loss           | 2.47e-09     |\n",
      "------------------------------------------\n",
      "438830 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "439999 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 86           |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 215          |\n",
      "|    time_elapsed         | 15817        |\n",
      "|    total_timesteps      | 440320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045714728 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.987       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0125       |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 9.54e-09     |\n",
      "------------------------------------------\n",
      "440563 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "441964 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 92.7        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 15897       |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008772767 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0242      |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 1.53e-09    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442826 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "443689 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99          |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 15977       |\n",
      "|    total_timesteps      | 444416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011416812 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | 0.000374    |\n",
      "|    value_loss           | 1.45e-08    |\n",
      "-----------------------------------------\n",
      "444762 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "445445 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 16057       |\n",
      "|    total_timesteps      | 446464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013623023 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 2.84e-06    |\n",
      "-----------------------------------------\n",
      "446901 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "447813 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 105          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 16138        |\n",
      "|    total_timesteps      | 448512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039046742 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.869       |\n",
      "|    explained_variance   | -2.31e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00271     |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | 0.000364     |\n",
      "|    value_loss           | 1.94e-06     |\n",
      "------------------------------------------\n",
      "448996 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "449857 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.7         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 220          |\n",
      "|    time_elapsed         | 16221        |\n",
      "|    total_timesteps      | 450560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077013504 |\n",
      "|    clip_fraction        | 0.234        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.906       |\n",
      "|    explained_variance   | -1.1e+12     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0135      |\n",
      "|    n_updates            | 2190         |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 1.89e-07     |\n",
      "------------------------------------------\n",
      "450481 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "451893 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 99.9          |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 221           |\n",
      "|    time_elapsed         | 16301         |\n",
      "|    total_timesteps      | 452608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0005877826 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.933        |\n",
      "|    explained_variance   | -7.8e+12      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0234        |\n",
      "|    n_updates            | 2200          |\n",
      "|    policy_gradient_loss | 0.000399      |\n",
      "|    value_loss           | 6.48e-07      |\n",
      "-------------------------------------------\n",
      "452999 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "453836 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 78.5        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 16383       |\n",
      "|    total_timesteps      | 454656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009276898 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.939      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00834     |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    value_loss           | 5.19e-06    |\n",
      "-----------------------------------------\n",
      "454780 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "455989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 74.2     |\n",
      "|    ep_rew_mean          | 0        |\n",
      "| time/                   |          |\n",
      "|    fps                  | 27       |\n",
      "|    iterations           | 223      |\n",
      "|    time_elapsed         | 16465    |\n",
      "|    total_timesteps      | 456704   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.010962 |\n",
      "|    clip_fraction        | 0.0312   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.862   |\n",
      "|    explained_variance   | nan      |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.00636 |\n",
      "|    n_updates            | 2220     |\n",
      "|    policy_gradient_loss | -0.0047  |\n",
      "|    value_loss           | 2.29e-08 |\n",
      "--------------------------------------\n",
      "456990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "457622 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 74.1        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 16546       |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004373147 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.905      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -7.21e-05   |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 4.07e-10    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458610 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "459965 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 78.5         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 225          |\n",
      "|    time_elapsed         | 16627        |\n",
      "|    total_timesteps      | 460800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020782202 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.867       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00599      |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.000983    |\n",
      "|    value_loss           | 1.81e-06     |\n",
      "------------------------------------------\n",
      "460905 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "461979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 90         |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 226        |\n",
      "|    time_elapsed         | 16707      |\n",
      "|    total_timesteps      | 462848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00804949 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.835     |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 2250       |\n",
      "|    policy_gradient_loss | -0.00295   |\n",
      "|    value_loss           | 8.11e-06   |\n",
      "----------------------------------------\n",
      "462990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "463987 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 95.3        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 16787       |\n",
      "|    total_timesteps      | 464896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002898363 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | -1.02e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0052     |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 6.83e-06    |\n",
      "-----------------------------------------\n",
      "464987 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "465976 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 228         |\n",
      "|    time_elapsed         | 16870       |\n",
      "|    total_timesteps      | 466944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010648561 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.858      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.000174    |\n",
      "|    value_loss           | 8.55e-06    |\n",
      "-----------------------------------------\n",
      "466815 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "467572 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 229          |\n",
      "|    time_elapsed         | 16949        |\n",
      "|    total_timesteps      | 468992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029904635 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.799       |\n",
      "|    explained_variance   | -7.51e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0275      |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.00436     |\n",
      "|    value_loss           | 3.97e-06     |\n",
      "------------------------------------------\n",
      "468650 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "469968 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "470870 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80.2        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 230         |\n",
      "|    time_elapsed         | 17032       |\n",
      "|    total_timesteps      | 471040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002193709 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.792      |\n",
      "|    explained_variance   | -6.52e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00835    |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 3.28e-06    |\n",
      "-----------------------------------------\n",
      "471963 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "472997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80.1        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 17115       |\n",
      "|    total_timesteps      | 473088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006420303 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | -7.7e+12    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000572    |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.00691    |\n",
      "|    value_loss           | 6.64e-07    |\n",
      "-----------------------------------------\n",
      "473847 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "474959 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 76.3         |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 17199        |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041803457 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.763       |\n",
      "|    explained_variance   | -5.69e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00503     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.00442     |\n",
      "|    value_loss           | 2.13e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475938 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "476851 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80          |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 17282       |\n",
      "|    total_timesteps      | 477184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004799155 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.744      |\n",
      "|    explained_variance   | -2.61e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 8.69e-08    |\n",
      "-----------------------------------------\n",
      "477994 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "478900 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96           |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 234          |\n",
      "|    time_elapsed         | 17365        |\n",
      "|    total_timesteps      | 479232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015461915 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.733       |\n",
      "|    explained_variance   | -2.74e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.029       |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -0.00587     |\n",
      "|    value_loss           | 2.3e-07      |\n",
      "------------------------------------------\n",
      "479990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "480740 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 235          |\n",
      "|    time_elapsed         | 17447        |\n",
      "|    total_timesteps      | 481280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044561396 |\n",
      "|    clip_fraction        | 0.188        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.692       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00512      |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 8.89e-07     |\n",
      "------------------------------------------\n",
      "481744 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "482601 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 112            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 27             |\n",
      "|    iterations           | 236            |\n",
      "|    time_elapsed         | 17530          |\n",
      "|    total_timesteps      | 483328         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00043638097 |\n",
      "|    clip_fraction        | 0.0156         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.735         |\n",
      "|    explained_variance   | -9.22e+11      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00451       |\n",
      "|    n_updates            | 2350           |\n",
      "|    policy_gradient_loss | -0.000166      |\n",
      "|    value_loss           | 1.93e-06       |\n",
      "--------------------------------------------\n",
      "483936 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "484948 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 237          |\n",
      "|    time_elapsed         | 17612        |\n",
      "|    total_timesteps      | 485376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027742612 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.722       |\n",
      "|    explained_variance   | -8.3e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00743      |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    value_loss           | 7.48e-08     |\n",
      "------------------------------------------\n",
      "485873 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "486465 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 135         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 17692       |\n",
      "|    total_timesteps      | 487424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007029816 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.729      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 1.35e-05    |\n",
      "-----------------------------------------\n",
      "487753 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "488552 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 148          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 239          |\n",
      "|    time_elapsed         | 17771        |\n",
      "|    total_timesteps      | 489472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029468215 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.714       |\n",
      "|    explained_variance   | -5.88e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0259       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.000608    |\n",
      "|    value_loss           | 1.11e-08     |\n",
      "------------------------------------------\n",
      "489753 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "490589 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 164        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 240        |\n",
      "|    time_elapsed         | 17850      |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01016189 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.68      |\n",
      "|    explained_variance   | -1.67e+12  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | -0.00323   |\n",
      "|    value_loss           | 8e-09      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491889 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "492969 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 180          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 241          |\n",
      "|    time_elapsed         | 17930        |\n",
      "|    total_timesteps      | 493568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033703241 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.71        |\n",
      "|    explained_variance   | -4.9e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0103      |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    value_loss           | 1.14e-08     |\n",
      "------------------------------------------\n",
      "493997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "494789 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 191          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 242          |\n",
      "|    time_elapsed         | 18009        |\n",
      "|    total_timesteps      | 495616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069389176 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | -8.08e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000756     |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    value_loss           | 7.24e-09     |\n",
      "------------------------------------------\n",
      "495749 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "496713 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 202          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 243          |\n",
      "|    time_elapsed         | 18087        |\n",
      "|    total_timesteps      | 497664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021546755 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.65        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0212       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    value_loss           | 3.94e-07     |\n",
      "------------------------------------------\n",
      "497945 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "498973 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 217          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 18166        |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053044157 |\n",
      "|    clip_fraction        | 0.391        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.712       |\n",
      "|    explained_variance   | -8.12e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0213      |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.005       |\n",
      "|    value_loss           | 1.51e-05     |\n",
      "------------------------------------------\n",
      "499885 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "500680 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 226          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 245          |\n",
      "|    time_elapsed         | 18244        |\n",
      "|    total_timesteps      | 501760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061467057 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.678       |\n",
      "|    explained_variance   | -5.78e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0227       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "501860 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "502900 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 233          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 246          |\n",
      "|    time_elapsed         | 18323        |\n",
      "|    total_timesteps      | 503808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025926146 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0133       |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    value_loss           | 1.3e-07      |\n",
      "------------------------------------------\n",
      "503876 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "504476 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 253          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 247          |\n",
      "|    time_elapsed         | 18403        |\n",
      "|    total_timesteps      | 505856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055665765 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.657       |\n",
      "|    explained_variance   | -5.42e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00327      |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 8.19e-08     |\n",
      "------------------------------------------\n",
      "505667 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "506935 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 261         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 18483       |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010649666 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | -3.34e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 3.8e-08     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507939 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "508995 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 271          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 249          |\n",
      "|    time_elapsed         | 18564        |\n",
      "|    total_timesteps      | 509952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031048765 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | -2.41e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00388      |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 1.04e-07     |\n",
      "------------------------------------------\n",
      "509467 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "510512 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "511456 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 281          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 250          |\n",
      "|    time_elapsed         | 18643        |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057797283 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.529       |\n",
      "|    explained_variance   | -2.79e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0153       |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00225     |\n",
      "|    value_loss           | 3.27e-08     |\n",
      "------------------------------------------\n",
      "512778 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "513414 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 294         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 18723       |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001451452 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | -4.28e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    value_loss           | 1.68e-06    |\n",
      "-----------------------------------------\n",
      "514818 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "515690 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 304          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 252          |\n",
      "|    time_elapsed         | 18803        |\n",
      "|    total_timesteps      | 516096       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027691503 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00587      |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "516634 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "517606 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 253          |\n",
      "|    time_elapsed         | 18886        |\n",
      "|    total_timesteps      | 518144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065090763 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.634       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00195      |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 8.73e-07     |\n",
      "------------------------------------------\n",
      "518978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "519706 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 303          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 254          |\n",
      "|    time_elapsed         | 18969        |\n",
      "|    total_timesteps      | 520192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009955575 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | -4.18e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00201      |\n",
      "|    n_updates            | 2530         |\n",
      "|    policy_gradient_loss | 0.000373     |\n",
      "|    value_loss           | 6.95e-06     |\n",
      "------------------------------------------\n",
      "520654 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "521978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 309           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 19052         |\n",
      "|    total_timesteps      | 522240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060658203 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.593        |\n",
      "|    explained_variance   | -8.1e+11      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00792      |\n",
      "|    n_updates            | 2540          |\n",
      "|    policy_gradient_loss | 1.37e-05      |\n",
      "|    value_loss           | 1.21e-06      |\n",
      "-------------------------------------------\n",
      "522958 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "523542 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 312          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 256          |\n",
      "|    time_elapsed         | 19137        |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051942756 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00793     |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 4.27e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524418 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "525802 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 320          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 257          |\n",
      "|    time_elapsed         | 19219        |\n",
      "|    total_timesteps      | 526336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074590007 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | -2.92e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0059      |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.00463     |\n",
      "|    value_loss           | 2.96e-07     |\n",
      "------------------------------------------\n",
      "526426 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "527878 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 318          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 258          |\n",
      "|    time_elapsed         | 19302        |\n",
      "|    total_timesteps      | 528384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017930245 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | -6.45e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00956      |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    value_loss           | 1.37e-07     |\n",
      "------------------------------------------\n",
      "528478 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "529678 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 323          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 259          |\n",
      "|    time_elapsed         | 19387        |\n",
      "|    total_timesteps      | 530432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023595423 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00489      |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 8.58e-07     |\n",
      "------------------------------------------\n",
      "530874 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "531962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 316         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 19467       |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003958608 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | -6.61e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00281    |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.000658   |\n",
      "|    value_loss           | 1.64e-07    |\n",
      "-----------------------------------------\n",
      "532998 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "533834 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 326          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 261          |\n",
      "|    time_elapsed         | 19545        |\n",
      "|    total_timesteps      | 534528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016099504 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.361       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0004      |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.00316     |\n",
      "|    value_loss           | 1.57e-07     |\n",
      "------------------------------------------\n",
      "534434 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "535962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 320          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 262          |\n",
      "|    time_elapsed         | 19624        |\n",
      "|    total_timesteps      | 536576       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016674794 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.312       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00333     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    value_loss           | 8.97e-07     |\n",
      "------------------------------------------\n",
      "536450 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "537966 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 323           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 263           |\n",
      "|    time_elapsed         | 19703         |\n",
      "|    total_timesteps      | 538624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027446402 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.318        |\n",
      "|    explained_variance   | -1.02e+12     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00021       |\n",
      "|    n_updates            | 2620          |\n",
      "|    policy_gradient_loss | -0.000522     |\n",
      "|    value_loss           | 3.96e-07      |\n",
      "-------------------------------------------\n",
      "538746 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "539678 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 322          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 264          |\n",
      "|    time_elapsed         | 19782        |\n",
      "|    total_timesteps      | 540672       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017118234 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00114     |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | 0.000157     |\n",
      "|    value_loss           | 1.9e-06      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540490 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "541466 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 330           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 265           |\n",
      "|    time_elapsed         | 19861         |\n",
      "|    total_timesteps      | 542720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010207901 |\n",
      "|    clip_fraction        | 0.0156        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.307        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.1e-05       |\n",
      "|    n_updates            | 2640          |\n",
      "|    policy_gradient_loss | -0.0021       |\n",
      "|    value_loss           | 1.06e-07      |\n",
      "-------------------------------------------\n",
      "542758 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "543958 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 342          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 266          |\n",
      "|    time_elapsed         | 19940        |\n",
      "|    total_timesteps      | 544768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023283162 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.341       |\n",
      "|    explained_variance   | -2.72e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00781      |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.000222    |\n",
      "|    value_loss           | 1.24e-06     |\n",
      "------------------------------------------\n",
      "544690 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "545934 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 345          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 267          |\n",
      "|    time_elapsed         | 20018        |\n",
      "|    total_timesteps      | 546816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013055896 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | -2.82e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00668     |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 9.91e-08     |\n",
      "------------------------------------------\n",
      "546538 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "547824 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 352          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 268          |\n",
      "|    time_elapsed         | 20097        |\n",
      "|    total_timesteps      | 548864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027939575 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.312       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00207     |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -0.000727    |\n",
      "|    value_loss           | 1.58e-07     |\n",
      "------------------------------------------\n",
      "548908 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "549708 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 355         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 269         |\n",
      "|    time_elapsed         | 20177       |\n",
      "|    total_timesteps      | 550912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003819466 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | -4.19e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00115    |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 2.21e-06    |\n",
      "-----------------------------------------\n",
      "550992 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "551916 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 363            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 27             |\n",
      "|    iterations           | 270            |\n",
      "|    time_elapsed         | 20255          |\n",
      "|    total_timesteps      | 552960         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00039708207 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.37          |\n",
      "|    explained_variance   | -5.63e+11      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.000469      |\n",
      "|    n_updates            | 2690           |\n",
      "|    policy_gradient_loss | -0.000339      |\n",
      "|    value_loss           | 2.93e-08       |\n",
      "--------------------------------------------\n",
      "552936 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "553952 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "554924 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 364          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 271          |\n",
      "|    time_elapsed         | 20336        |\n",
      "|    total_timesteps      | 555008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047013145 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | -3.52e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00506     |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 6.79e-08     |\n",
      "------------------------------------------\n",
      "555520 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "556720 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 371        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 272        |\n",
      "|    time_elapsed         | 20415      |\n",
      "|    total_timesteps      | 557056     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00277603 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.47      |\n",
      "|    explained_variance   | -4.6e+12   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 2710       |\n",
      "|    policy_gradient_loss | -2.04e-05  |\n",
      "|    value_loss           | 3.16e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557920 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "558912 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 369         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 273         |\n",
      "|    time_elapsed         | 20494       |\n",
      "|    total_timesteps      | 559104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007018964 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | -6.47e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00406    |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    value_loss           | 4.21e-09    |\n",
      "-----------------------------------------\n",
      "559772 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "560752 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 374          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 274          |\n",
      "|    time_elapsed         | 20573        |\n",
      "|    total_timesteps      | 561152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037070857 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.606       |\n",
      "|    explained_variance   | -5.96e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0016       |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 1.43e-08     |\n",
      "------------------------------------------\n",
      "561952 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "562612 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 380          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 275          |\n",
      "|    time_elapsed         | 20654        |\n",
      "|    total_timesteps      | 563200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006033266 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0163       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 6.3e-06      |\n",
      "------------------------------------------\n",
      "563780 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "564672 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 376          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 276          |\n",
      "|    time_elapsed         | 20738        |\n",
      "|    total_timesteps      | 565248       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026189955 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0243      |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | -0.000146    |\n",
      "|    value_loss           | 4.04e-10     |\n",
      "------------------------------------------\n",
      "565872 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "566986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 381           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 277           |\n",
      "|    time_elapsed         | 20819         |\n",
      "|    total_timesteps      | 567296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084938155 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.573        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00916       |\n",
      "|    n_updates            | 2760          |\n",
      "|    policy_gradient_loss | 0.000493      |\n",
      "|    value_loss           | 4.04e-10      |\n",
      "-------------------------------------------\n",
      "567802 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "568762 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 373          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 278          |\n",
      "|    time_elapsed         | 20902        |\n",
      "|    total_timesteps      | 569344       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005778086 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.634       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0082      |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.000708    |\n",
      "|    value_loss           | 1.75e-09     |\n",
      "------------------------------------------\n",
      "569986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "570862 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 377           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 20985         |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5702967e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.648        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00311      |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | -3.05e-05     |\n",
      "|    value_loss           | 1.3e-08       |\n",
      "-------------------------------------------\n",
      "571802 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "572742 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 377          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 280          |\n",
      "|    time_elapsed         | 21068        |\n",
      "|    total_timesteps      | 573440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015737796 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.716       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00809     |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 1.58e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573786 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "574986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 389         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 281         |\n",
      "|    time_elapsed         | 21152       |\n",
      "|    total_timesteps      | 575488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002737015 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.774      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00205    |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.000417   |\n",
      "|    value_loss           | 3.43e-08    |\n",
      "-----------------------------------------\n",
      "575586 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "576786 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 282          |\n",
      "|    time_elapsed         | 21234        |\n",
      "|    total_timesteps      | 577536       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054024295 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.81        |\n",
      "|    explained_variance   | -1.89e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00474     |\n",
      "|    n_updates            | 2810         |\n",
      "|    policy_gradient_loss | -0.000368    |\n",
      "|    value_loss           | 1.96e-06     |\n",
      "------------------------------------------\n",
      "577426 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "578626 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 392          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 283          |\n",
      "|    time_elapsed         | 21313        |\n",
      "|    total_timesteps      | 579584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013432463 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.759       |\n",
      "|    explained_variance   | -4.37e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00462     |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | 0.00046      |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "579844 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "580692 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 392          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 284          |\n",
      "|    time_elapsed         | 21392        |\n",
      "|    total_timesteps      | 581632       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066437568 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.674       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.000989    |\n",
      "|    value_loss           | 1.61e-05     |\n",
      "------------------------------------------\n",
      "582000 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "582600 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 389           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 285           |\n",
      "|    time_elapsed         | 21473         |\n",
      "|    total_timesteps      | 583680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086878124 |\n",
      "|    clip_fraction        | 0.0156        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.606        |\n",
      "|    explained_variance   | -3.46e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0317        |\n",
      "|    n_updates            | 2840          |\n",
      "|    policy_gradient_loss | -0.000212     |\n",
      "|    value_loss           | 2.71e-08      |\n",
      "-------------------------------------------\n",
      "583598 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "584774 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 385          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 286          |\n",
      "|    time_elapsed         | 21552        |\n",
      "|    total_timesteps      | 585728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018538204 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.532       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00658     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 5.59e-09     |\n",
      "------------------------------------------\n",
      "585762 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "586842 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 381            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 27             |\n",
      "|    iterations           | 287            |\n",
      "|    time_elapsed         | 21632          |\n",
      "|    total_timesteps      | 587776         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00027881213 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.475         |\n",
      "|    explained_variance   | -2.44e+11      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00108       |\n",
      "|    n_updates            | 2860           |\n",
      "|    policy_gradient_loss | 8.54e-05       |\n",
      "|    value_loss           | 5.32e-09       |\n",
      "--------------------------------------------\n",
      "587906 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "588574 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 388          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 288          |\n",
      "|    time_elapsed         | 21711        |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013520956 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.465       |\n",
      "|    explained_variance   | -5.46e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0127       |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.000475    |\n",
      "|    value_loss           | 2.61e-09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589710 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "590834 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 386          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 21791        |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035715566 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00507     |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 2.04e-09     |\n",
      "------------------------------------------\n",
      "591562 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "592978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 390          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 290          |\n",
      "|    time_elapsed         | 21869        |\n",
      "|    total_timesteps      | 593920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010249643 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | -3.46e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000864    |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | 0.000613     |\n",
      "|    value_loss           | 3.2e-09      |\n",
      "------------------------------------------\n",
      "593578 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "594523 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 21948       |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002577379 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000513    |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.000635   |\n",
      "|    value_loss           | 9.65e-10    |\n",
      "-----------------------------------------\n",
      "595963 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "596555 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "597975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 392          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 292          |\n",
      "|    time_elapsed         | 22028        |\n",
      "|    total_timesteps      | 598016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015639225 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0022      |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 7.04e-09     |\n",
      "------------------------------------------\n",
      "598975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "599895 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 379         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 293         |\n",
      "|    time_elapsed         | 22107       |\n",
      "|    total_timesteps      | 600064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002880306 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000516   |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 1.22e-06    |\n",
      "-----------------------------------------\n",
      "600839 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "601731 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 382         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 294         |\n",
      "|    time_elapsed         | 22186       |\n",
      "|    total_timesteps      | 602112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002555437 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | -4.98e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00216    |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 7.09e-08    |\n",
      "-----------------------------------------\n",
      "602947 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "603744 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 377           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 295           |\n",
      "|    time_elapsed         | 22266         |\n",
      "|    total_timesteps      | 604160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2581385e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.252        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00182      |\n",
      "|    n_updates            | 2940          |\n",
      "|    policy_gradient_loss | -0.000818     |\n",
      "|    value_loss           | 3.55e-07      |\n",
      "-------------------------------------------\n",
      "604926 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "605674 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 364         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 296         |\n",
      "|    time_elapsed         | 22345       |\n",
      "|    total_timesteps      | 606208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002168224 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00785     |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 4.13e-07    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606786 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "607954 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 355          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 297          |\n",
      "|    time_elapsed         | 22425        |\n",
      "|    total_timesteps      | 608256       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001912301 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | -6.36e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00111     |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | 2.38e-06     |\n",
      "|    value_loss           | 9.49e-07     |\n",
      "------------------------------------------\n",
      "608502 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "609986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 350           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 298           |\n",
      "|    time_elapsed         | 22505         |\n",
      "|    total_timesteps      | 610304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031031272 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.3          |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00237       |\n",
      "|    n_updates            | 2970          |\n",
      "|    policy_gradient_loss | 0.000125      |\n",
      "|    value_loss           | 1.21e-08      |\n",
      "-------------------------------------------\n",
      "610586 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "611990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 350          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 299          |\n",
      "|    time_elapsed         | 22584        |\n",
      "|    total_timesteps      | 612352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011080211 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00997      |\n",
      "|    n_updates            | 2980         |\n",
      "|    policy_gradient_loss | -0.000752    |\n",
      "|    value_loss           | 1.51e-06     |\n",
      "------------------------------------------\n",
      "612938 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "613538 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 343          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 300          |\n",
      "|    time_elapsed         | 22663        |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011415838 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | -2.03e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00233      |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    value_loss           | 5.5e-09      |\n",
      "------------------------------------------\n",
      "614646 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "615886 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 329          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 301          |\n",
      "|    time_elapsed         | 22743        |\n",
      "|    total_timesteps      | 616448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021438994 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00606     |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 5.75e-09     |\n",
      "------------------------------------------\n",
      "616950 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "617550 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "---------------------------------------------\n",
      "| rollout/                |                 |\n",
      "|    ep_len_mean          | 335             |\n",
      "|    ep_rew_mean          | 0               |\n",
      "| time/                   |                 |\n",
      "|    fps                  | 27              |\n",
      "|    iterations           | 302             |\n",
      "|    time_elapsed         | 22822           |\n",
      "|    total_timesteps      | 618496          |\n",
      "| train/                  |                 |\n",
      "|    approx_kl            | -0.000101008336 |\n",
      "|    clip_fraction        | 0               |\n",
      "|    clip_range           | 0.2             |\n",
      "|    entropy_loss         | -0.317          |\n",
      "|    explained_variance   | nan             |\n",
      "|    learning_rate        | 0.0003          |\n",
      "|    loss                 | 0.00291         |\n",
      "|    n_updates            | 3010            |\n",
      "|    policy_gradient_loss | -0.000178       |\n",
      "|    value_loss           | 2.95e-06        |\n",
      "---------------------------------------------\n",
      "618811 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "619927 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 340          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 303          |\n",
      "|    time_elapsed         | 22900        |\n",
      "|    total_timesteps      | 620544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011675218 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00235     |\n",
      "|    n_updates            | 3020         |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 3.9e-11      |\n",
      "------------------------------------------\n",
      "620763 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "621815 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 345         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 22980       |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001661465 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00246     |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.000949   |\n",
      "|    value_loss           | 6.09e-11    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622415 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "623937 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 330          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 305          |\n",
      "|    time_elapsed         | 23062        |\n",
      "|    total_timesteps      | 624640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022822383 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.365       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00451     |\n",
      "|    n_updates            | 3040         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 8.28e-11     |\n",
      "------------------------------------------\n",
      "624585 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "625917 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 327          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 306          |\n",
      "|    time_elapsed         | 23145        |\n",
      "|    total_timesteps      | 626688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006288921 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.32        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00805      |\n",
      "|    n_updates            | 3050         |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 1.1e-06      |\n",
      "------------------------------------------\n",
      "626849 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "627797 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 325         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 307         |\n",
      "|    time_elapsed         | 23226       |\n",
      "|    total_timesteps      | 628736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005599692 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 1.99e-11    |\n",
      "-----------------------------------------\n",
      "628433 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "629633 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 330           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 308           |\n",
      "|    time_elapsed         | 23308         |\n",
      "|    total_timesteps      | 630784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090986094 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.26         |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -6.94e-05     |\n",
      "|    n_updates            | 3070          |\n",
      "|    policy_gradient_loss | 7.65e-05      |\n",
      "|    value_loss           | 4.52e-07      |\n",
      "-------------------------------------------\n",
      "630929 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "631741 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 338            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 27             |\n",
      "|    iterations           | 309            |\n",
      "|    time_elapsed         | 23389          |\n",
      "|    total_timesteps      | 632832         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00021458114 |\n",
      "|    clip_fraction        | 0.125          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.28          |\n",
      "|    explained_variance   | nan            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00281       |\n",
      "|    n_updates            | 3080           |\n",
      "|    policy_gradient_loss | -8.72e-05      |\n",
      "|    value_loss           | 3.44e-07       |\n",
      "--------------------------------------------\n",
      "632785 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "633709 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 347           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 310           |\n",
      "|    time_elapsed         | 23471         |\n",
      "|    total_timesteps      | 634880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033019355 |\n",
      "|    clip_fraction        | 0.0156        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.329        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00273      |\n",
      "|    n_updates            | 3090          |\n",
      "|    policy_gradient_loss | -0.000243     |\n",
      "|    value_loss           | 6.72e-06      |\n",
      "-------------------------------------------\n",
      "634909 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "635953 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 336          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 311          |\n",
      "|    time_elapsed         | 23555        |\n",
      "|    total_timesteps      | 636928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048120283 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 3.76e-11     |\n",
      "------------------------------------------\n",
      "636941 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "637541 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 342          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 312          |\n",
      "|    time_elapsed         | 23637        |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015379597 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.263       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00572     |\n",
      "|    n_updates            | 3110         |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 5.5e-11      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "639709 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "640979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 354           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 313           |\n",
      "|    time_elapsed         | 23717         |\n",
      "|    total_timesteps      | 641024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0017606235 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.299        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00814       |\n",
      "|    n_updates            | 3120          |\n",
      "|    policy_gradient_loss | 0.000119      |\n",
      "|    value_loss           | 3.38e-11      |\n",
      "-------------------------------------------\n",
      "641471 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "642563 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 363           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 314           |\n",
      "|    time_elapsed         | 23796         |\n",
      "|    total_timesteps      | 643072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071142777 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.286        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00168       |\n",
      "|    n_updates            | 3130          |\n",
      "|    policy_gradient_loss | -0.00167      |\n",
      "|    value_loss           | 5.62e-11      |\n",
      "-------------------------------------------\n",
      "643475 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "644755 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 359          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 315          |\n",
      "|    time_elapsed         | 23874        |\n",
      "|    total_timesteps      | 645120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001788662 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.013       |\n",
      "|    n_updates            | 3140         |\n",
      "|    policy_gradient_loss | -0.000593    |\n",
      "|    value_loss           | 5.41e-11     |\n",
      "------------------------------------------\n",
      "645567 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "646975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 359          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 316          |\n",
      "|    time_elapsed         | 23954        |\n",
      "|    total_timesteps      | 647168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010597875 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00277      |\n",
      "|    n_updates            | 3150         |\n",
      "|    policy_gradient_loss | -0.00019     |\n",
      "|    value_loss           | 1.15e-07     |\n",
      "------------------------------------------\n",
      "647611 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "648875 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 359          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 317          |\n",
      "|    time_elapsed         | 24033        |\n",
      "|    total_timesteps      | 649216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013117297 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | -1.49e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00143      |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 2.29e-06     |\n",
      "------------------------------------------\n",
      "649763 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "650959 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 357          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 318          |\n",
      "|    time_elapsed         | 24113        |\n",
      "|    total_timesteps      | 651264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024985312 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.341       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0203      |\n",
      "|    n_updates            | 3170         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 2.99e-07     |\n",
      "------------------------------------------\n",
      "651473 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "652637 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 370          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 319          |\n",
      "|    time_elapsed         | 24191        |\n",
      "|    total_timesteps      | 653312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020000802 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00751     |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 2.42e-07     |\n",
      "------------------------------------------\n",
      "653985 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "654833 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 357          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 320          |\n",
      "|    time_elapsed         | 24271        |\n",
      "|    total_timesteps      | 655360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006592084 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | -9.16e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00287     |\n",
      "|    n_updates            | 3190         |\n",
      "|    policy_gradient_loss | -5.91e-05    |\n",
      "|    value_loss           | 1.61e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655969 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "656797 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 366         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 24349       |\n",
      "|    total_timesteps      | 657408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001724539 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | 6.51e-05    |\n",
      "|    value_loss           | 5.61e-07    |\n",
      "-----------------------------------------\n",
      "657997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "658741 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 361            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 26             |\n",
      "|    iterations           | 322            |\n",
      "|    time_elapsed         | 24429          |\n",
      "|    total_timesteps      | 659456         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00013405946 |\n",
      "|    clip_fraction        | 0.109          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.496         |\n",
      "|    explained_variance   | -4.22e+11      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00378       |\n",
      "|    n_updates            | 3210           |\n",
      "|    policy_gradient_loss | -0.0018        |\n",
      "|    value_loss           | 5.74e-07       |\n",
      "--------------------------------------------\n",
      "659957 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "660849 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 366           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 323           |\n",
      "|    time_elapsed         | 24507         |\n",
      "|    total_timesteps      | 661504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076660945 |\n",
      "|    clip_fraction        | 0.0625        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.54         |\n",
      "|    explained_variance   | -7.17e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00394       |\n",
      "|    n_updates            | 3220          |\n",
      "|    policy_gradient_loss | -0.00082      |\n",
      "|    value_loss           | 9.14e-08      |\n",
      "-------------------------------------------\n",
      "661685 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "662981 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 371          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 324          |\n",
      "|    time_elapsed         | 24586        |\n",
      "|    total_timesteps      | 663552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029212204 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.583       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0205       |\n",
      "|    n_updates            | 3230         |\n",
      "|    policy_gradient_loss | -0.000441    |\n",
      "|    value_loss           | 2.38e-06     |\n",
      "------------------------------------------\n",
      "663581 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "664640 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 365          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 325          |\n",
      "|    time_elapsed         | 24664        |\n",
      "|    total_timesteps      | 665600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035360362 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0215      |\n",
      "|    n_updates            | 3240         |\n",
      "|    policy_gradient_loss | -0.00246     |\n",
      "|    value_loss           | 1.77e-09     |\n",
      "------------------------------------------\n",
      "665554 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "666922 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 366          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 326          |\n",
      "|    time_elapsed         | 24744        |\n",
      "|    total_timesteps      | 667648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015575462 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00483      |\n",
      "|    n_updates            | 3250         |\n",
      "|    policy_gradient_loss | 0.000399     |\n",
      "|    value_loss           | 5.27e-11     |\n",
      "------------------------------------------\n",
      "667522 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "668582 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 365         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 327         |\n",
      "|    time_elapsed         | 24823       |\n",
      "|    total_timesteps      | 669696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003232364 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    value_loss           | 1.59e-08    |\n",
      "-----------------------------------------\n",
      "669570 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "670818 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 359          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 328          |\n",
      "|    time_elapsed         | 24901        |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035950365 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 3270         |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 2.48e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671714 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "672914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 329          |\n",
      "|    time_elapsed         | 24981        |\n",
      "|    total_timesteps      | 673792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035686917 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0118      |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    value_loss           | 1.53e-09     |\n",
      "------------------------------------------\n",
      "673514 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "674714 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 373         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 25060       |\n",
      "|    total_timesteps      | 675840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007855652 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | -3.48e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 9.85e-09    |\n",
      "-----------------------------------------\n",
      "675914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "676794 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 376         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 25138       |\n",
      "|    total_timesteps      | 677888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001568692 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | -3.37e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00545    |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.000156   |\n",
      "|    value_loss           | 1.75e-06    |\n",
      "-----------------------------------------\n",
      "677786 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "678734 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 384          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 332          |\n",
      "|    time_elapsed         | 25218        |\n",
      "|    total_timesteps      | 679936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011743206 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.631       |\n",
      "|    explained_variance   | -4.19e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00338     |\n",
      "|    n_updates            | 3310         |\n",
      "|    policy_gradient_loss | 0.000222     |\n",
      "|    value_loss           | 3.95e-09     |\n",
      "------------------------------------------\n",
      "679834 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "680990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 383         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 333         |\n",
      "|    time_elapsed         | 25295       |\n",
      "|    total_timesteps      | 681984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003155339 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00673     |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | 0.000564    |\n",
      "|    value_loss           | 8.78e-08    |\n",
      "-----------------------------------------\n",
      "681994 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "682854 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "683828 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 334          |\n",
      "|    time_elapsed         | 25376        |\n",
      "|    total_timesteps      | 684032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038953829 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.7         |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0187       |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -0.000362    |\n",
      "|    value_loss           | 1.22e-05     |\n",
      "------------------------------------------\n",
      "684704 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "685692 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 371          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 335          |\n",
      "|    time_elapsed         | 25455        |\n",
      "|    total_timesteps      | 686080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019813362 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.712       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00284      |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -8.99e-05    |\n",
      "|    value_loss           | 2.46e-10     |\n",
      "------------------------------------------\n",
      "686742 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "687942 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 378           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 336           |\n",
      "|    time_elapsed         | 25541         |\n",
      "|    total_timesteps      | 688128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00080363045 |\n",
      "|    clip_fraction        | 0.0469        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.676        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0049       |\n",
      "|    n_updates            | 3350          |\n",
      "|    policy_gradient_loss | -0.000546     |\n",
      "|    value_loss           | 1.23e-10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688542 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "689946 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 381         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 337         |\n",
      "|    time_elapsed         | 25624       |\n",
      "|    total_timesteps      | 690176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009260802 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.744      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00994     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    value_loss           | 1.76e-10    |\n",
      "-----------------------------------------\n",
      "690950 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "691911 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 379         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 25709       |\n",
      "|    total_timesteps      | 692224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015877377 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.836      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000755   |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    value_loss           | 1.27e-10    |\n",
      "-----------------------------------------\n",
      "692847 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "693862 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 379         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 339         |\n",
      "|    time_elapsed         | 25793       |\n",
      "|    total_timesteps      | 694272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008938308 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 8.68e-09    |\n",
      "-----------------------------------------\n",
      "694734 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "695706 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 380        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 340        |\n",
      "|    time_elapsed         | 25877      |\n",
      "|    total_timesteps      | 696320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01997089 |\n",
      "|    clip_fraction        | 0.0469     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.845     |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0318    |\n",
      "|    n_updates            | 3390       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 2.51e-07   |\n",
      "----------------------------------------\n",
      "696918 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "697518 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 341          |\n",
      "|    time_elapsed         | 25962        |\n",
      "|    total_timesteps      | 698368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019101909 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.849       |\n",
      "|    explained_variance   | -1.77e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000937     |\n",
      "|    n_updates            | 3400         |\n",
      "|    policy_gradient_loss | 0.000351     |\n",
      "|    value_loss           | 6.17e-07     |\n",
      "------------------------------------------\n",
      "698622 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "699951 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 377         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 342         |\n",
      "|    time_elapsed         | 26044       |\n",
      "|    total_timesteps      | 700416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005719334 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.895      |\n",
      "|    explained_variance   | -3.71e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0165      |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.000371   |\n",
      "|    value_loss           | 1.73e-06    |\n",
      "-----------------------------------------\n",
      "700551 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "701843 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 343          |\n",
      "|    time_elapsed         | 26124        |\n",
      "|    total_timesteps      | 702464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018062737 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.878       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0106       |\n",
      "|    n_updates            | 3420         |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    value_loss           | 7.75e-09     |\n",
      "------------------------------------------\n",
      "702443 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "703457 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 379         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 26202       |\n",
      "|    total_timesteps      | 704512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016669668 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.823      |\n",
      "|    explained_variance   | -7.26e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00665    |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    value_loss           | 6.6e-09     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704826 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "705961 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 370          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 345          |\n",
      "|    time_elapsed         | 26281        |\n",
      "|    total_timesteps      | 706560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010541349 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.8         |\n",
      "|    explained_variance   | -2.49e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00254     |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.000828    |\n",
      "|    value_loss           | 3.85e-06     |\n",
      "------------------------------------------\n",
      "706952 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "707908 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 370          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 346          |\n",
      "|    time_elapsed         | 26360        |\n",
      "|    total_timesteps      | 708608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043410566 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.746       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0282       |\n",
      "|    n_updates            | 3450         |\n",
      "|    policy_gradient_loss | -0.00298     |\n",
      "|    value_loss           | 2.28e-09     |\n",
      "------------------------------------------\n",
      "708566 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "709813 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 353         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 347         |\n",
      "|    time_elapsed         | 26439       |\n",
      "|    total_timesteps      | 710656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004699679 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    value_loss           | 8.68e-10    |\n",
      "-----------------------------------------\n",
      "710860 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "711812 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 342         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 348         |\n",
      "|    time_elapsed         | 26517       |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002257067 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00718     |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 1.6e-09     |\n",
      "-----------------------------------------\n",
      "712545 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "713896 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 329          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 349          |\n",
      "|    time_elapsed         | 26596        |\n",
      "|    total_timesteps      | 714752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029126434 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.683       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    value_loss           | 4.88e-07     |\n",
      "------------------------------------------\n",
      "714629 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "715993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 287         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 26677       |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009837844 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00379     |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.00442    |\n",
      "|    value_loss           | 1.98e-07    |\n",
      "-----------------------------------------\n",
      "716953 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "717972 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 234          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 351          |\n",
      "|    time_elapsed         | 26759        |\n",
      "|    total_timesteps      | 718848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028543617 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | -1.89e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00278     |\n",
      "|    n_updates            | 3500         |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    value_loss           | 1.01e-06     |\n",
      "------------------------------------------\n",
      "718715 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "719971 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 190          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 352          |\n",
      "|    time_elapsed         | 26839        |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035792082 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | -6.58e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00116     |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 5.39e-08     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720822 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "721914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 137          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 353          |\n",
      "|    time_elapsed         | 26919        |\n",
      "|    total_timesteps      | 722944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025788448 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.599       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000754    |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 4.15e-07     |\n",
      "------------------------------------------\n",
      "722853 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "723947 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 124           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 354           |\n",
      "|    time_elapsed         | 27000         |\n",
      "|    total_timesteps      | 724992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00093483855 |\n",
      "|    clip_fraction        | 0.0156        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.57         |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0165       |\n",
      "|    n_updates            | 3530          |\n",
      "|    policy_gradient_loss | -0.00451      |\n",
      "|    value_loss           | 7.1e-07       |\n",
      "-------------------------------------------\n",
      "724936 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "725988 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "726446 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 105          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 355          |\n",
      "|    time_elapsed         | 27082        |\n",
      "|    total_timesteps      | 727040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023437217 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.62        |\n",
      "|    explained_variance   | -1.79e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00369      |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 3.09e-06     |\n",
      "------------------------------------------\n",
      "727834 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "728991 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 113          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 356          |\n",
      "|    time_elapsed         | 27163        |\n",
      "|    total_timesteps      | 729088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030289404 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.64        |\n",
      "|    explained_variance   | -9.03e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00924     |\n",
      "|    n_updates            | 3550         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 5.31e-08     |\n",
      "------------------------------------------\n",
      "729797 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "730924 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 123         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 357         |\n",
      "|    time_elapsed         | 27242       |\n",
      "|    total_timesteps      | 731136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004495218 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | -1.55e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00158    |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    value_loss           | 3.22e-08    |\n",
      "-----------------------------------------\n",
      "731954 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "732658 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 124          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 358          |\n",
      "|    time_elapsed         | 27322        |\n",
      "|    total_timesteps      | 733184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023685475 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.621       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.018        |\n",
      "|    n_updates            | 3570         |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 1.3e-08      |\n",
      "------------------------------------------\n",
      "733848 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "734975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 137          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 359          |\n",
      "|    time_elapsed         | 27402        |\n",
      "|    total_timesteps      | 735232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039025692 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00416     |\n",
      "|    n_updates            | 3580         |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    value_loss           | 2.02e-07     |\n",
      "------------------------------------------\n",
      "735834 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "736542 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 147         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 360         |\n",
      "|    time_elapsed         | 27483       |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002578773 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00444    |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.00448    |\n",
      "|    value_loss           | 9.31e-07    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737932 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "738637 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 160          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 361          |\n",
      "|    time_elapsed         | 27562        |\n",
      "|    total_timesteps      | 739328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037302063 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.587       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00153      |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 1.48e-08     |\n",
      "------------------------------------------\n",
      "739402 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "740612 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 362          |\n",
      "|    time_elapsed         | 27641        |\n",
      "|    total_timesteps      | 741376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026198374 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.607       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00496     |\n",
      "|    n_updates            | 3610         |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    value_loss           | 1.31e-06     |\n",
      "------------------------------------------\n",
      "741551 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "742855 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 178          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 363          |\n",
      "|    time_elapsed         | 27722        |\n",
      "|    total_timesteps      | 743424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027028196 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.619       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000641     |\n",
      "|    n_updates            | 3620         |\n",
      "|    policy_gradient_loss | 2.86e-05     |\n",
      "|    value_loss           | 2.29e-10     |\n",
      "------------------------------------------\n",
      "743826 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "744646 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 190          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 364          |\n",
      "|    time_elapsed         | 27801        |\n",
      "|    total_timesteps      | 745472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025135772 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.606       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0175      |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 2.07e-07     |\n",
      "------------------------------------------\n",
      "745599 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "746627 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 196         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 365         |\n",
      "|    time_elapsed         | 27880       |\n",
      "|    total_timesteps      | 747520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005865184 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00723    |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.000916   |\n",
      "|    value_loss           | 4.75e-08    |\n",
      "-----------------------------------------\n",
      "747957 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "748749 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 208       |\n",
      "|    ep_rew_mean          | 0         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 26        |\n",
      "|    iterations           | 366       |\n",
      "|    time_elapsed         | 27961     |\n",
      "|    total_timesteps      | 749568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0050315 |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.524    |\n",
      "|    explained_variance   | nan       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0256   |\n",
      "|    n_updates            | 3650      |\n",
      "|    policy_gradient_loss | -0.00195  |\n",
      "|    value_loss           | 7.14e-07  |\n",
      "---------------------------------------\n",
      "749843 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "750949 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 207          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 367          |\n",
      "|    time_elapsed         | 28042        |\n",
      "|    total_timesteps      | 751616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005611852 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.479       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.00511     |\n",
      "|    value_loss           | 2.68e-07     |\n",
      "------------------------------------------\n",
      "751978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "752508 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 203           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 368           |\n",
      "|    time_elapsed         | 28123         |\n",
      "|    total_timesteps      | 753664        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097534235 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.456        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0054       |\n",
      "|    n_updates            | 3670          |\n",
      "|    policy_gradient_loss | -0.00282      |\n",
      "|    value_loss           | 6.43e-07      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "754819 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 193          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 369          |\n",
      "|    time_elapsed         | 28203        |\n",
      "|    total_timesteps      | 755712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027252226 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | -1.84e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00175      |\n",
      "|    n_updates            | 3680         |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    value_loss           | 9.72e-08     |\n",
      "------------------------------------------\n",
      "755978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "756803 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 156          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 370          |\n",
      "|    time_elapsed         | 28285        |\n",
      "|    total_timesteps      | 757760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020099464 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.19e-06     |\n",
      "|    n_updates            | 3690         |\n",
      "|    policy_gradient_loss | -0.00411     |\n",
      "|    value_loss           | 2.02e-06     |\n",
      "------------------------------------------\n",
      "757618 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "758778 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 135          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 371          |\n",
      "|    time_elapsed         | 28365        |\n",
      "|    total_timesteps      | 759808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019280701 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00175     |\n",
      "|    n_updates            | 3700         |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 1.1e-09      |\n",
      "------------------------------------------\n",
      "759965 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "760703 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 129          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 372          |\n",
      "|    time_elapsed         | 28445        |\n",
      "|    total_timesteps      | 761856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008610197 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00883     |\n",
      "|    n_updates            | 3710         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 1.48e-09     |\n",
      "------------------------------------------\n",
      "761691 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "762975 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.5        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 28533       |\n",
      "|    total_timesteps      | 763904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003341733 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0266      |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.000599   |\n",
      "|    value_loss           | 2.32e-08    |\n",
      "-----------------------------------------\n",
      "763857 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "764978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 103          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 374          |\n",
      "|    time_elapsed         | 28616        |\n",
      "|    total_timesteps      | 765952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029688482 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0142       |\n",
      "|    n_updates            | 3730         |\n",
      "|    policy_gradient_loss | -0.00469     |\n",
      "|    value_loss           | 1.83e-06     |\n",
      "------------------------------------------\n",
      "765935 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "766697 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "767978 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 104         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 375         |\n",
      "|    time_elapsed         | 28701       |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001003549 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 1.59e-09    |\n",
      "-----------------------------------------\n",
      "768940 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "769728 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 104          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 376          |\n",
      "|    time_elapsed         | 28783        |\n",
      "|    total_timesteps      | 770048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036657918 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.432       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00558     |\n",
      "|    n_updates            | 3750         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 2.88e-09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770972 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "771926 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 108            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 26             |\n",
      "|    iterations           | 377            |\n",
      "|    time_elapsed         | 28867          |\n",
      "|    total_timesteps      | 772096         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00073519803 |\n",
      "|    clip_fraction        | 0.172          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.439         |\n",
      "|    explained_variance   | nan            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 0.0181         |\n",
      "|    n_updates            | 3760           |\n",
      "|    policy_gradient_loss | -0.00202       |\n",
      "|    value_loss           | 1.69e-06       |\n",
      "--------------------------------------------\n",
      "772963 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "773843 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 119         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 28950       |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002185509 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00173    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 2.77e-10    |\n",
      "-----------------------------------------\n",
      "774951 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "775974 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 126          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 379          |\n",
      "|    time_elapsed         | 29034        |\n",
      "|    total_timesteps      | 776192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027700432 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.000522    |\n",
      "|    value_loss           | 6.38e-11     |\n",
      "------------------------------------------\n",
      "776996 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "777853 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 132          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 380          |\n",
      "|    time_elapsed         | 29117        |\n",
      "|    total_timesteps      | 778240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007697111 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.437       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0145       |\n",
      "|    n_updates            | 3790         |\n",
      "|    policy_gradient_loss | -0.000238    |\n",
      "|    value_loss           | 1.91e-06     |\n",
      "------------------------------------------\n",
      "778997 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "779671 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 127         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 381         |\n",
      "|    time_elapsed         | 29201       |\n",
      "|    total_timesteps      | 780288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003868274 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | -1.49e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 4.17e-08    |\n",
      "-----------------------------------------\n",
      "780864 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "781845 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 128         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 29285       |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000693355 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | -1.04e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0027     |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 3.45e-08    |\n",
      "-----------------------------------------\n",
      "782877 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "783959 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 383          |\n",
      "|    time_elapsed         | 29369        |\n",
      "|    total_timesteps      | 784384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018053764 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | -1.72e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00121      |\n",
      "|    n_updates            | 3820         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 2.81e-07     |\n",
      "------------------------------------------\n",
      "784838 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "785968 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 384          |\n",
      "|    time_elapsed         | 29451        |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025918772 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | -1.36e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00109      |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 5.46e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786788 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "787972 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 385          |\n",
      "|    time_elapsed         | 29534        |\n",
      "|    total_timesteps      | 788480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028070393 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00247      |\n",
      "|    n_updates            | 3840         |\n",
      "|    policy_gradient_loss | -0.000739    |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "788932 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "789897 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 126           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 386           |\n",
      "|    time_elapsed         | 29617         |\n",
      "|    total_timesteps      | 790528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0005647235 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.329        |\n",
      "|    explained_variance   | -1.41e+12     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00237       |\n",
      "|    n_updates            | 3850          |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    value_loss           | 1.34e-06      |\n",
      "-------------------------------------------\n",
      "790925 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "791938 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 132           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 387           |\n",
      "|    time_elapsed         | 29698         |\n",
      "|    total_timesteps      | 792576        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0013267815 |\n",
      "|    clip_fraction        | 0.0469        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.346        |\n",
      "|    explained_variance   | -6.53e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00705       |\n",
      "|    n_updates            | 3860          |\n",
      "|    policy_gradient_loss | -0.00133      |\n",
      "|    value_loss           | 2.23e-08      |\n",
      "-------------------------------------------\n",
      "792756 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "793949 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 135          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 388          |\n",
      "|    time_elapsed         | 29778        |\n",
      "|    total_timesteps      | 794624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032215044 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.321       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00299     |\n",
      "|    n_updates            | 3870         |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 6.4e-08      |\n",
      "------------------------------------------\n",
      "794977 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "795965 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 147          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 389          |\n",
      "|    time_elapsed         | 29858        |\n",
      "|    total_timesteps      | 796672       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023455522 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | -4.7e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000197    |\n",
      "|    n_updates            | 3880         |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 1.56e-07     |\n",
      "------------------------------------------\n",
      "796881 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "797855 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 390          |\n",
      "|    time_elapsed         | 29937        |\n",
      "|    total_timesteps      | 798720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016483751 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | -7.75e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00105      |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 6.73e-07     |\n",
      "------------------------------------------\n",
      "798721 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "799872 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 391          |\n",
      "|    time_elapsed         | 30017        |\n",
      "|    total_timesteps      | 800768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014305469 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | -2.9e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00034     |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 6.38e-08     |\n",
      "------------------------------------------\n",
      "800912 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "801666 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 30097       |\n",
      "|    total_timesteps      | 802816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000491648 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | -3.08e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00437    |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.000936   |\n",
      "|    value_loss           | 8.34e-07    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802831 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "803787 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 180         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 393         |\n",
      "|    time_elapsed         | 30176       |\n",
      "|    total_timesteps      | 804864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003010724 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | -2.58e+12   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00338     |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.00376    |\n",
      "|    value_loss           | 7.24e-08    |\n",
      "-----------------------------------------\n",
      "804531 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "805791 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 191          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 394          |\n",
      "|    time_elapsed         | 30257        |\n",
      "|    total_timesteps      | 806912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014918178 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | -4.57e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00352      |\n",
      "|    n_updates            | 3930         |\n",
      "|    policy_gradient_loss | -0.000189    |\n",
      "|    value_loss           | 1.84e-06     |\n",
      "------------------------------------------\n",
      "806939 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "807663 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 206          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 395          |\n",
      "|    time_elapsed         | 30335        |\n",
      "|    total_timesteps      | 808960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015323252 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | -7.57e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00418     |\n",
      "|    n_updates            | 3940         |\n",
      "|    policy_gradient_loss | -0.000192    |\n",
      "|    value_loss           | 3.95e-09     |\n",
      "------------------------------------------\n",
      "808957 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "809965 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "810914 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 220          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 396          |\n",
      "|    time_elapsed         | 30415        |\n",
      "|    total_timesteps      | 811008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021559969 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | -4.96e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000695    |\n",
      "|    n_updates            | 3950         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    value_loss           | 2.05e-09     |\n",
      "------------------------------------------\n",
      "811770 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "812939 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 223           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 397           |\n",
      "|    time_elapsed         | 30495         |\n",
      "|    total_timesteps      | 813056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048347458 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.529        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0029        |\n",
      "|    n_updates            | 3960          |\n",
      "|    policy_gradient_loss | -0.0013       |\n",
      "|    value_loss           | 1.85e-06      |\n",
      "-------------------------------------------\n",
      "813933 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "814664 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 232          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 398          |\n",
      "|    time_elapsed         | 30575        |\n",
      "|    total_timesteps      | 815104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018993865 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | -9.9e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000639     |\n",
      "|    n_updates            | 3970         |\n",
      "|    policy_gradient_loss | -0.000688    |\n",
      "|    value_loss           | 3.28e-07     |\n",
      "------------------------------------------\n",
      "815918 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "816538 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 246          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 399          |\n",
      "|    time_elapsed         | 30657        |\n",
      "|    total_timesteps      | 817152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038608112 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0257      |\n",
      "|    n_updates            | 3980         |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 8.55e-08     |\n",
      "------------------------------------------\n",
      "817819 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "818952 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 254          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 400          |\n",
      "|    time_elapsed         | 30738        |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036383402 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | -7.66e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00123     |\n",
      "|    n_updates            | 3990         |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    value_loss           | 5.84e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819865 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "821000 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 248          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 401          |\n",
      "|    time_elapsed         | 30818        |\n",
      "|    total_timesteps      | 821248       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022294638 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | -5.46e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00505      |\n",
      "|    n_updates            | 4000         |\n",
      "|    policy_gradient_loss | -0.00439     |\n",
      "|    value_loss           | 2.24e-07     |\n",
      "------------------------------------------\n",
      "821985 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "822987 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 227            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 26             |\n",
      "|    iterations           | 402            |\n",
      "|    time_elapsed         | 30898          |\n",
      "|    total_timesteps      | 823296         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00029235112 |\n",
      "|    clip_fraction        | 0.0469         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.514         |\n",
      "|    explained_variance   | -6.26e+12      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00357       |\n",
      "|    n_updates            | 4010           |\n",
      "|    policy_gradient_loss | -0.00432       |\n",
      "|    value_loss           | 5.91e-07       |\n",
      "--------------------------------------------\n",
      "823996 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "824765 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 229          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 403          |\n",
      "|    time_elapsed         | 30979        |\n",
      "|    total_timesteps      | 825344       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028228597 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | -8.59e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00968      |\n",
      "|    n_updates            | 4020         |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    value_loss           | 1.55e-07     |\n",
      "------------------------------------------\n",
      "825981 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "826868 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 204          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 404          |\n",
      "|    time_elapsed         | 31061        |\n",
      "|    total_timesteps      | 827392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048219985 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.421       |\n",
      "|    explained_variance   | -6.69e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00522      |\n",
      "|    n_updates            | 4030         |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 1.39e-07     |\n",
      "------------------------------------------\n",
      "827843 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "828962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 173          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 405          |\n",
      "|    time_elapsed         | 31142        |\n",
      "|    total_timesteps      | 829440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015673216 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.426       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00693      |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    value_loss           | 2.41e-07     |\n",
      "------------------------------------------\n",
      "829979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "830912 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 155          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 406          |\n",
      "|    time_elapsed         | 31222        |\n",
      "|    total_timesteps      | 831488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011485078 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.434       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00584      |\n",
      "|    n_updates            | 4050         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 8.28e-07     |\n",
      "------------------------------------------\n",
      "831962 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "832953 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 135            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 26             |\n",
      "|    iterations           | 407            |\n",
      "|    time_elapsed         | 31303          |\n",
      "|    total_timesteps      | 833536         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00017998996 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.455         |\n",
      "|    explained_variance   | nan            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.00298       |\n",
      "|    n_updates            | 4060           |\n",
      "|    policy_gradient_loss | -0.000662      |\n",
      "|    value_loss           | 2.06e-08       |\n",
      "--------------------------------------------\n",
      "833990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "834986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 132            |\n",
      "|    ep_rew_mean          | 0              |\n",
      "| time/                   |                |\n",
      "|    fps                  | 26             |\n",
      "|    iterations           | 408            |\n",
      "|    time_elapsed         | 31384          |\n",
      "|    total_timesteps      | 835584         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00020800019 |\n",
      "|    clip_fraction        | 0.0312         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.435         |\n",
      "|    explained_variance   | nan            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 0.00996        |\n",
      "|    n_updates            | 4070           |\n",
      "|    policy_gradient_loss | -0.00129       |\n",
      "|    value_loss           | 2.91e-07       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835972 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "836666 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 136           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 409           |\n",
      "|    time_elapsed         | 31465         |\n",
      "|    total_timesteps      | 837632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075719436 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.417        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00846      |\n",
      "|    n_updates            | 4080          |\n",
      "|    policy_gradient_loss | -0.0011       |\n",
      "|    value_loss           | 7.54e-07      |\n",
      "-------------------------------------------\n",
      "837599 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "838953 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 137          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 410          |\n",
      "|    time_elapsed         | 31549        |\n",
      "|    total_timesteps      | 839680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029952016 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00514     |\n",
      "|    n_updates            | 4090         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 6.54e-08     |\n",
      "------------------------------------------\n",
      "839877 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "840817 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 411          |\n",
      "|    time_elapsed         | 31635        |\n",
      "|    total_timesteps      | 841728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018367214 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.005       |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 3.27e-07     |\n",
      "------------------------------------------\n",
      "841613 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "842912 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 143         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 31719       |\n",
      "|    total_timesteps      | 843776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001801968 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | -9.91e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00642     |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 4.86e-07    |\n",
      "-----------------------------------------\n",
      "843877 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "844948 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 157          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 413          |\n",
      "|    time_elapsed         | 31802        |\n",
      "|    total_timesteps      | 845824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028888267 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.293       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00139      |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 1.21e-07     |\n",
      "------------------------------------------\n",
      "845822 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "846917 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 154           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 414           |\n",
      "|    time_elapsed         | 31888         |\n",
      "|    total_timesteps      | 847872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072804803 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.333        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00429      |\n",
      "|    n_updates            | 4130          |\n",
      "|    policy_gradient_loss | -0.000345     |\n",
      "|    value_loss           | 4.69e-07      |\n",
      "-------------------------------------------\n",
      "847606 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "848829 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 415         |\n",
      "|    time_elapsed         | 31972       |\n",
      "|    total_timesteps      | 849920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002801442 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.325      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00425    |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 1.42e-07    |\n",
      "-----------------------------------------\n",
      "849725 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "850979 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 176           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 416           |\n",
      "|    time_elapsed         | 32055         |\n",
      "|    total_timesteps      | 851968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042468915 |\n",
      "|    clip_fraction        | 0.0469        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.292        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00389       |\n",
      "|    n_updates            | 4150          |\n",
      "|    policy_gradient_loss | -0.000471     |\n",
      "|    value_loss           | 2.19e-07      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851818 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "852934 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "853831 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 168           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 417           |\n",
      "|    time_elapsed         | 32139         |\n",
      "|    total_timesteps      | 854016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060999685 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.289        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.000457     |\n",
      "|    n_updates            | 4160          |\n",
      "|    policy_gradient_loss | -0.00195      |\n",
      "|    value_loss           | 9.65e-07      |\n",
      "-------------------------------------------\n",
      "854760 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "855716 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 167           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 418           |\n",
      "|    time_elapsed         | 32225         |\n",
      "|    total_timesteps      | 856064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010861177 |\n",
      "|    clip_fraction        | 0.0156        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.286        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.000722     |\n",
      "|    n_updates            | 4170          |\n",
      "|    policy_gradient_loss | -0.00166      |\n",
      "|    value_loss           | 6.57e-09      |\n",
      "-------------------------------------------\n",
      "856989 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "857514 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 161          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 419          |\n",
      "|    time_elapsed         | 32310        |\n",
      "|    total_timesteps      | 858112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023406348 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.313       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00334      |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 3e-06        |\n",
      "------------------------------------------\n",
      "858804 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "859798 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 159          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 420          |\n",
      "|    time_elapsed         | 32395        |\n",
      "|    total_timesteps      | 860160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021844404 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.321       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00426      |\n",
      "|    n_updates            | 4190         |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 1.03e-07     |\n",
      "------------------------------------------\n",
      "860964 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "861984 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 156          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 421          |\n",
      "|    time_elapsed         | 32480        |\n",
      "|    total_timesteps      | 862208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019332715 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00034      |\n",
      "|    n_updates            | 4200         |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 6.21e-08     |\n",
      "------------------------------------------\n",
      "862732 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "863993 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 164         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 32566       |\n",
      "|    total_timesteps      | 864256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002104817 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | -7e+12      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00165     |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    value_loss           | 5.04e-08    |\n",
      "-----------------------------------------\n",
      "864461 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "865750 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 423          |\n",
      "|    time_elapsed         | 32650        |\n",
      "|    total_timesteps      | 866304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035117755 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | -6.84e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00315      |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 8.2e-08      |\n",
      "------------------------------------------\n",
      "866930 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "867619 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 175          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 424          |\n",
      "|    time_elapsed         | 32730        |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005764783 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | -4.71e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00283      |\n",
      "|    n_updates            | 4230         |\n",
      "|    policy_gradient_loss | -0.000345    |\n",
      "|    value_loss           | 1.48e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868991 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "869855 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 425          |\n",
      "|    time_elapsed         | 32810        |\n",
      "|    total_timesteps      | 870400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015527143 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00521     |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    value_loss           | 2.09e-07     |\n",
      "------------------------------------------\n",
      "870852 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "871990 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 184           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 426           |\n",
      "|    time_elapsed         | 32890         |\n",
      "|    total_timesteps      | 872448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034044147 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.231        |\n",
      "|    explained_variance   | -1.83e+12     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000255      |\n",
      "|    n_updates            | 4250          |\n",
      "|    policy_gradient_loss | -0.000317     |\n",
      "|    value_loss           | 2.99e-07      |\n",
      "-------------------------------------------\n",
      "872986 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "873950 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 193          |\n",
      "|    ep_rew_mean          | 0            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 427          |\n",
      "|    time_elapsed         | 32969        |\n",
      "|    total_timesteps      | 874496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018634953 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0188      |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 3.24e-07     |\n",
      "------------------------------------------\n",
      "874857 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "875845 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 206         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 428         |\n",
      "|    time_elapsed         | 33047       |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001351391 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.221      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 3.43e-07    |\n",
      "-----------------------------------------\n",
      "876549 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "877880 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 215           |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 26            |\n",
      "|    iterations           | 429           |\n",
      "|    time_elapsed         | 33130         |\n",
      "|    total_timesteps      | 878592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015076622 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.228        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00024       |\n",
      "|    n_updates            | 4280          |\n",
      "|    policy_gradient_loss | -0.000398     |\n",
      "|    value_loss           | 6.45e-07      |\n",
      "-------------------------------------------\n",
      "878882 timesteps\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.00\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0e9d03917d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/visionpack/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    239\u001b[0m                                       \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                       \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                                       eval_log_path=eval_log_path, reset_num_timesteps=reset_num_timesteps)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/visionpack/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    192\u001b[0m             continue_training = self.collect_rollouts(self.env, callback,\n\u001b[1;32m    193\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                                                       n_rollout_steps=self.n_steps)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/visionpack/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/visionpack/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/visionpack/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/visionpack/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/GVGAI_GYM/gym_gvgai/envs/gvgai_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misOver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGVGAI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# print(state.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"new_\"+\"1e6\"+\"_treasurekeeper\"+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 51823 ...\n",
      "Client connected to server [OK]\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gym_gvgai\n",
    "import os\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from stable_baselines3 import PPO  # test arbitrary agent\n",
    "\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "  \"\"\"\n",
    "  Callback called at each step (for DQN and others) or after n steps (see ACER or PPO2)\n",
    "  :param _locals: (dict)\n",
    "  :param _globals: (dict)\n",
    "  \"\"\"\n",
    "  global n_steps, best_mean_reward\n",
    "  if (n_steps + 1) % 1000 == 0:\n",
    "      # Evaluate policy training performance\n",
    "#       if (n_steps + 1) % 10000 == 0:\n",
    "#           _locals['self'].save(log_dir + 'checkpoint'+str(n_steps)+'.pkl')\n",
    "      x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "      if len(x) > 0:\n",
    "          mean_reward = np.mean(y[-100:])\n",
    "          print(x[-1], 'timesteps')\n",
    "          print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "          # New best model, you could save the agent here\n",
    "          if mean_reward > best_mean_reward:\n",
    "              best_mean_reward = mean_reward\n",
    "              # Example for saving best model\n",
    "              print(\"Saving new best model\")\n",
    "              _locals['self'].save(log_dir + 'best_model_waterpuzzle_lvl0.pkl')\n",
    "  n_steps += 1\n",
    "  return True\n",
    "\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"./\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = gym_gvgai.make('gvgai-waterpuzzle-lvl0-v0')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=1)\n",
    "\n",
    "# model = PPO(CnnPolicy, env, verbose=1,prioritized_replay=True, buffer_size= 1000000, exploration_final_eps=0.1, train_freq=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 timesteps\n",
      "Best mean reward: -inf - Last mean reward per episode: 5.00\n",
      "Saving new best model\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 5            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014897749 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00523     |\n",
      "|    n_updates            | 4290         |\n",
      "|    policy_gradient_loss | -0.000603    |\n",
      "|    value_loss           | 1.38e-08     |\n",
      "------------------------------------------\n",
      "3000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 5.00\n",
      "3000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 5.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 5           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014369739 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -24         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "4500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 3.33\n",
      "6000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 2.50\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 2.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006652833 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -2.95e+13   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000133   |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "6000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 2.50\n",
      "7500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 2.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 2           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 470         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008623746 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "9000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.67\n",
      "9000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 1.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018353552 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | -1.19e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00677    |\n",
      "|    value_loss           | 0.000497    |\n",
      "-----------------------------------------\n",
      "10500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.43\n",
      "12000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.25\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.5e+03    |\n",
      "|    ep_rew_mean          | 1.25       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 718        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02192364 |\n",
      "|    clip_fraction        | 0.469      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | nan        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0295     |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.00775   |\n",
      "|    value_loss           | 0.000356   |\n",
      "----------------------------------------\n",
      "12000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.25\n",
      "13500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.11\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 845         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012979539 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -1.12e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000362   |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00262    |\n",
      "|    value_loss           | 0.00045     |\n",
      "-----------------------------------------\n",
      "15000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.00\n",
      "15000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 1.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.5e+03    |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 972        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02144957 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.32      |\n",
      "|    explained_variance   | -1.57e+11  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00485   |\n",
      "|    value_loss           | 0.000164   |\n",
      "----------------------------------------\n",
      "16500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1099        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013802831 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0203     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    value_loss           | 0.000114    |\n",
      "-----------------------------------------\n",
      "18000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.83\n",
      "19500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1225        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003184093 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -5.66e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00543    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.000238    |\n",
      "|    value_loss           | 0.000145    |\n",
      "-----------------------------------------\n",
      "21000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.71\n",
      "21000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.71\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1354        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010714202 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.978      |\n",
      "|    explained_variance   | -1.95e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0099     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    value_loss           | 5.24e-05    |\n",
      "-----------------------------------------\n",
      "22500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.67\n",
      "24000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.62\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 16           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 1486         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050235577 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.91        |\n",
      "|    explained_variance   | -1.04e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00827      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    value_loss           | 6.99e-05     |\n",
      "------------------------------------------\n",
      "24000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.62\n",
      "25500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.59\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.588       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1650        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006530826 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.813      |\n",
      "|    explained_variance   | -8.43e+10   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0181      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000639   |\n",
      "|    value_loss           | 2.6e-05     |\n",
      "-----------------------------------------\n",
      "27000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.56\n",
      "27000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.56\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.526        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 1816         |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032630828 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.741       |\n",
      "|    explained_variance   | -2.2e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00233     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | 0.00043      |\n",
      "|    value_loss           | 1.62e-05     |\n",
      "------------------------------------------\n",
      "28500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.53\n",
      "30000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.50\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 1995         |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016939187 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.743       |\n",
      "|    explained_variance   | -1.12e+12    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000238    |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000192    |\n",
      "|    value_loss           | 2.32e-05     |\n",
      "------------------------------------------\n",
      "30000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.50\n",
      "31500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.71\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.714       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2173        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004215088 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.769      |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0867      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00452    |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.68\n",
      "33000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 2365        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005344201 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | -2.03e+11   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00146    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.000569   |\n",
      "|    value_loss           | 1.03e-05    |\n",
      "-----------------------------------------\n",
      "34500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.65\n",
      "36000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.62\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2562        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003071676 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | -2.7e+11    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00241     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.000762    |\n",
      "|    value_loss           | 1.04e-06    |\n",
      "-----------------------------------------\n",
      "36000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.62\n",
      "37500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.60\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.5e+03       |\n",
      "|    ep_rew_mean          | 0.6           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 2751          |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044010032 |\n",
      "|    clip_fraction        | 0.0469        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.585        |\n",
      "|    explained_variance   | -9.57e+10     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00285      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000468     |\n",
      "|    value_loss           | 3.9e-07       |\n",
      "-------------------------------------------\n",
      "39000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.58\n",
      "39000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.58\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.556        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 2987         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064832997 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00612     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 2.7e-07      |\n",
      "------------------------------------------\n",
      "40500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.56\n",
      "42000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.54\n",
      "42000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.54\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.536        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 3167         |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031063422 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | -1.5e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00901     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000776    |\n",
      "|    value_loss           | 3.26e-07     |\n",
      "------------------------------------------\n",
      "43500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.52\n",
      "45000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.50\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 3349         |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031478908 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.48        |\n",
      "|    explained_variance   | -4.62e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00205     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 1.2e-07      |\n",
      "------------------------------------------\n",
      "45000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.50\n",
      "46500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.48\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.484        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 3516         |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027545844 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.417       |\n",
      "|    explained_variance   | -6.11e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00279     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 0.000124     |\n",
      "|    value_loss           | 1.53e-07     |\n",
      "------------------------------------------\n",
      "48000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.47\n",
      "48000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.47\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.469        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 3685         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023822263 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00121     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 5.77e-08     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.45\n",
      "51000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.44\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.441        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 3862         |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036552707 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | -1.4e+11     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0014      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    value_loss           | 3.87e-08     |\n",
      "------------------------------------------\n",
      "51000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.44\n",
      "52500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.43\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.429        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 4031         |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033618775 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | -1.87e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00263      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 4.91e-08     |\n",
      "------------------------------------------\n",
      "54000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.42\n",
      "54000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.42\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.417        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 4199         |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018062398 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | -2.45e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000188     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000503    |\n",
      "|    value_loss           | 1.74e-08     |\n",
      "------------------------------------------\n",
      "55500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.41\n",
      "57000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.39\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.395        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 4371         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035516638 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | -6.33e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000138    |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000467    |\n",
      "|    value_loss           | 1.13e-08     |\n",
      "------------------------------------------\n",
      "57000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.39\n",
      "58500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.38\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.385        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 4550         |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014798004 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | -8.29e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00204     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000142    |\n",
      "|    value_loss           | 1.51e-08     |\n",
      "------------------------------------------\n",
      "60000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.38\n",
      "60000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.38\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.5e+03       |\n",
      "|    ep_rew_mean          | 0.375         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 12            |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 4728          |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0004267078 |\n",
      "|    clip_fraction        | 0.0312        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.181        |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00107      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000726     |\n",
      "|    value_loss           | 6.19e-09      |\n",
      "-------------------------------------------\n",
      "61500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.37\n",
      "63000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.36\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.357        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 4900         |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016602194 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000366    |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    value_loss           | 4.02e-09     |\n",
      "------------------------------------------\n",
      "63000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.36\n",
      "64500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.35\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.349        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 5077         |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005557304 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | -2.47e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000365     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000237    |\n",
      "|    value_loss           | 5.42e-09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.34\n",
      "66000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.34\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 1.5e+03        |\n",
      "|    ep_rew_mean          | 0.333          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 12             |\n",
      "|    iterations           | 33             |\n",
      "|    time_elapsed         | 5246           |\n",
      "|    total_timesteps      | 67584          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -1.3862737e-06 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.14          |\n",
      "|    explained_variance   | nan            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 0.00044        |\n",
      "|    n_updates            | 320            |\n",
      "|    policy_gradient_loss | -2.49e-05      |\n",
      "|    value_loss           | 2.16e-09       |\n",
      "--------------------------------------------\n",
      "67500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.33\n",
      "69000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.33\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.326        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 5413         |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030059116 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | -1.17e+11    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -6.8e-09     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 2.71e-09     |\n",
      "------------------------------------------\n",
      "69000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.33\n",
      "70500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.32\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.319        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 5578         |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006974394 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | -4.17e+10    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000333     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000408    |\n",
      "|    value_loss           | 1.23e-09     |\n",
      "------------------------------------------\n",
      "72000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.31\n",
      "72000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.31\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 0.306        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 5754         |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008971398 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000926    |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000414    |\n",
      "|    value_loss           | 1.53e-09     |\n",
      "------------------------------------------\n",
      "73500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.31\n",
      "75000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.30\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.5e+03       |\n",
      "|    ep_rew_mean          | 0.3           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 12            |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 5940          |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0012046103 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0812       |\n",
      "|    explained_variance   | nan           |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.92e-07      |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.000308     |\n",
      "|    value_loss           | 4.07e-07      |\n",
      "-------------------------------------------\n",
      "75000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.30\n",
      "76500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.29\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.5e+03       |\n",
      "|    ep_rew_mean          | 0.294         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 12            |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 6110          |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017589284 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0755       |\n",
      "|    explained_variance   | -1.04e+11     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.000995     |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.000564     |\n",
      "|    value_loss           | 3.09e-06      |\n",
      "-------------------------------------------\n",
      "78000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.29\n",
      "78000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.29\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 1.5e+03        |\n",
      "|    ep_rew_mean          | 0.283          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 12             |\n",
      "|    iterations           | 39             |\n",
      "|    time_elapsed         | 6290           |\n",
      "|    total_timesteps      | 79872          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00026789098 |\n",
      "|    clip_fraction        | 0.0156         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.065         |\n",
      "|    explained_variance   | -5.63e+10      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | -0.000407      |\n",
      "|    n_updates            | 380            |\n",
      "|    policy_gradient_loss | -0.000153      |\n",
      "|    value_loss           | 3.8e-09        |\n",
      "--------------------------------------------\n",
      "79500 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.28\n",
      "81000 timesteps\n",
      "Best mean reward: 5.00 - Last mean reward per episode: 0.28\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e6), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"new_\"+\"1e6\"+\"_waterpuzzle\"+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stateObs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8e3e7770bf3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstateObs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stateObs' is not defined"
     ]
    }
   ],
   "source": [
    "stateObs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJfxfzSIPtgV"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_UPeEL4fjJI"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_gvgai\n",
    "import Agent as Agent\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_state(env, step, name, info):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (name, step, info))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "%matplotlib inline\n",
    "env = gym_gvgai.make('gvgai-aliens' + '-' + 'lvl0-v0')\n",
    "agent = Agent.Agent()\n",
    "stateObs = env.reset()\n",
    "actions = env.unwrapped.get_action_meanings()\n",
    "score = 0\n",
    "for  i in range(1000):\n",
    "    show_state(env, i, 'Aliens', str(score))\n",
    "    action_id = agent.act(stateObs, actions)\n",
    "    stateObs, diffScore, done, debug = env.step(action_id)\n",
    "    score += diffScore\n",
    "    if done:\n",
    "        break\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel/miniconda3/lib/python3.7/site-packages/stable_baselines/common/input.py:30: RuntimeWarning: overflow encountered in subtract\n",
      "  np.any((ob_space.high - ob_space.low) != 0)):\n",
      "/home/joel/miniconda3/lib/python3.7/site-packages/stable_baselines/common/input.py:33: RuntimeWarning: overflow encountered in subtract\n",
      "  processed_observations = ((processed_observations - ob_space.low) / (ob_space.high - ob_space.low))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f3466a9862c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_early_resets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCnnPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprioritized_replay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_final_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, gamma, learning_rate, buffer_size, exploration_fraction, exploration_final_eps, exploration_initial_eps, train_freq, batch_size, double_q, learning_starts, target_network_update_freq, prioritized_replay, prioritized_replay_alpha, prioritized_replay_beta0, prioritized_replay_beta_iters, prioritized_replay_eps, param_noise, n_cpu_tf_sess, verbose, tensorboard_log, _init_setup_model, policy_kwargs, full_tensorboard_log, seed)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pretrain_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mfull_tensorboard_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_tensorboard_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mdouble_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 )\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py\u001b[0m in \u001b[0;36mbuild_train\u001b[0;34m(q_func, ob_space, ac_space, optimizer, sess, grad_norm_clipping, gamma, double_q, scope, reuse, param_noise, param_noise_filter_func, full_tensorboard_log)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                                         param_noise_filter_func=param_noise_filter_func)\n\u001b[1;32m    366\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mact_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_phs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochastic_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_eps_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# q network evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py\u001b[0m in \u001b[0;36mbuild_act\u001b[0;34m(q_func, ob_space, ac_space, stochastic_ph, update_eps_ph, sess)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mobs_phs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mdeterministic_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/deepq/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse, obs_phs, dueling, **_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         super(CnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n\u001b[1;32m    175\u001b[0m                                         \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_phs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs_phs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdueling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdueling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                                         layer_norm=False, **_kwargs)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/deepq/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse, layers, cnn_extractor, feature_extraction, obs_phs, layer_norm, dueling, act_fun, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfeature_extraction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0mextracted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0maction_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextracted_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36mnature_cnn\u001b[0;34m(scaled_images, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[1;32m     24\u001b[0m     \u001b[0mactiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mlayer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mlayer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlayer_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py\u001b[0m in \u001b[0;36mconv\u001b[0;34m(input_tensor, scope, n_filters, filter_size, stride, pad, init_scale, data_format, one_dim_bias)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mbias_var_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mone_dim_bias\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel_ax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mwshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilter_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    868\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines import DQN\n",
    "\n",
    "n_cpu = 8\n",
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "model = DQN(CnnPolicy, env, verbose=1,prioritized_replay=True, buffer_size= 1000000, exploration_final_eps=0.1, train_freq=4)\n",
    "model.learn(total_timesteps=int(1e10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMWnUHv3oRMpQ0KelBGSXcU",
   "mount_file_id": "1RPmGLYcFB0-HVuMCgaMlgYRfTOy4MB_J",
   "name": "train.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
